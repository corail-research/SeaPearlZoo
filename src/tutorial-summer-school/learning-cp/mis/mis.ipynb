{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS\n",
    "In a graph, the maximal independent set, also known as maximal stable set is an independent set that is not a subset of any other independent set. In other words, it is a set of vertices such that no two vertices in the set are adjacent. In this notebook, we will build a learning-based model to find the maximal independent set in a graph.\n",
    "\n",
    "<img src=\"img/Independent_set_graph.png\" alt=\"Solved MIS\" style=\"width:400px; height:400px;\">\n",
    "\n",
    "SeaPearl currently supports learning-based value selection models trained with Reinforcement Learning. To train Reinforcement Learning agents, we start by generating training instances and we let the agent learn a value selection heuristic. Like all other Reinforcement Learning tasks, we need to define a reward function, a state representation and an action space. Finally, we also need to build a neural network that will learn the value selection heuristic. All of these components will be defined in the following sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will begin by activating the environment and importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\leobo\\Desktop\\Ã‰cole\\Poly\\SeaPearl\\SeaPearlZoo.jl`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReinforcementLearning"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using Pkg\n",
    "Pkg.activate(\"../../../../\")\n",
    "Pkg.instantiate()\n",
    "using SeaPearl\n",
    "using Flux\n",
    "using LightGraphs\n",
    "using Random\n",
    "using BSON: @save, @load\n",
    "using ReinforcementLearning\n",
    "using CSV\n",
    "const RL = ReinforcementLearning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating instances\n",
    "\n",
    "SeaPearl provides a number of instance generators, including one for the MIS problem. Under the hood, this generator creates Barabasi-Albert graphs with `n` vertices. The graphs are grown by adding new vertices to an initial that has `k` vertices. New vertices are connected by `k` edges to `k` different vertices already present in the system by preferential attachment. The resulting graphs are undirected.\n",
    "\n",
    "<img src=\"img/450px-Barabasi_albert_graph.png\" alt=\"Barabasi-Albert Graph\" style=\"width:600px; height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.MaximumIndependentSetGenerator(8, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numInitialVertices = 3\n",
    "numNewVertices = 8\n",
    "instance_generator = SeaPearl.MaximumIndependentSetGenerator(numNewVertices, numInitialVertices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning setup\n",
    "\n",
    "### The state representation\n",
    "In SeaPearl, the state $s_t$ is defined as a pair $s_t = (P_t, x_t)$, with $P_t$ a partially solved combinatorial optimization problem and $x_t$ a variable selected at time $t$ of an episode. A terminal episode is reached if all variables are fixed or if a failure is detected.\n",
    "\n",
    "### The action space\n",
    "Given a state $s_t = (P_t, x_t)$, an action $a_t$ represents the selection of a value $v$ for the variable $x_t$. The action space is defined as the set of all possible values for the variable $x_t$ at time $t$.\n",
    "\n",
    "### The transition function\n",
    "Given a state $s_t = (P_t, x_t)$ and an action $a_t = v$, the transition function is comprised of three steps;\n",
    "1. The value of variable $x_{267}$ is assigned as $v$ (i.e., $D(x_{t+1}) = v$).\n",
    "2. The fix-point operation is applied on $P_t$ to prune the domains (i.e., $P_{t+1} = \\text{{fixPoint}}(P_t)$).\n",
    "3. The next variable to branch on is selected (i.e., $x_{t+1} = \\text{{nextVariable}}(P_{t+1})$).\n",
    "This results in a new state $s_{t+1} = (P_{t+1}, x_{t+1})$.\n",
    "\n",
    "\n",
    "### The reward function\n",
    "SeaPearl uses a \"propagation-based reward\". As the goal of the agent is to quickly find a good solution, it needs to learn to effectively prune the search space and move toward promising regions. An intuitive way to configure the reward is to give the agent the objective value, but this information is only available at the end of episodes. In other words, it makes the reward signal extremely sparse. To address this problem, SeaPearl uses both an intermediate reward and a final reward. The intuition behind the intermediate reward is this: it is computed by rewarding the pruning of high values from the variable's domain and penalizing the pruning of low values from the variable's domain. Mathematcially, the intermediate reward is defined as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "r_t^{ub} = \\{ v \\in D_t(x_{\\text{{obj}}}) \\mid v \\notin D_{t+1}(x_{\\text{{obj}}}) \\land v > \\max(D_t(x_{\\text{{obj}}})) \\} \\\\\n",
    "r_t^{lb} = \\{ v \\in D_t(x_{\\text{{obj}}}) \\mid v \\notin D_{t+1}(x_{\\text{{obj}}}) \\land v < \\min(D_t(x_{\\text{{obj}}})) \\} \\\\\n",
    "r^{mid}_t = \\frac{{r_t^{ub} - r_t^{lb}}}{{\\lvert D_1(x_{\\text{{obj}}}) \\rvert}} \\\\\n",
    "r^{end}_t = \\begin{cases} -1 & \\text{{if unfeasible solution found}} \\\\ 0 & \\text{{otherwise}} \\end{cases} \\\\\n",
    "r_{acc} = \\frac{{\\sum_{t=1}^{T} (r^{mid}_t + r^{end}_t)}}{{T-1}}\n",
    "$$\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We will now begin to implement the MIS problem in SeaPearl. To start, we will define the reward, which comes directly from the mathematical definition above. It is implemented in the `GeneralReward` of SeaPearl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.GeneralReward"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = SeaPearl.GeneralReward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network\n",
    "\n",
    "Next up, we need to define the neural network that will learn the value selection function. As the problems can differ in size, the use of graph neural networks (GNNs) is particularly appropriate. GNNs are a class of neural networks that operate on graphs, which means we need to convert the problem instances to graphs. In SeaPearl, we use tripartite graphs, which are graphs with three types of nodes: variables, values and constraints. There is one node for every variable, for every value and for every constraint. The edges are defined as follows:\n",
    " - There is an edge between a variable and a value if the value is in the domain of the variable.\n",
    " - There is an edge between a variable and a constraint if the variable appears in the constraint.\n",
    "\n",
    "Nodes have the following features:\n",
    " - Values have a one-hot encoding of their value. For example, if the domain of a variable is $\\{1, 2, 3\\}$, then the values will have a one-hot encoding of $[1, 2, 3]$.\n",
    " - Constraints have a one-hot encoding of their type (i.e., constraint).\n",
    "\n",
    "Other features can be used in the graph and we will define a featurization for it later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the experiment\n",
    "\n",
    "In the next cell, we will set up the experiment. We will create structs for the agent and the experiment. We will also define the hyperparameters of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MisExperimentSettings(300, 1, 10, 10, 1, 8, 3, 123)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"MisAgentConfig holds parameters for the configuration of the RL agent that will be used\"\"\"\n",
    "\n",
    "struct MisAgentConfig\n",
    "    gamma::Float32  # Discount factor for future rewards\n",
    "    batch_size::Int  # Batch size\n",
    "    output_size::Int  # One for every node\n",
    "    update_horizon::Int  # How many steps to look ahead in the environment\n",
    "    min_replay_history::Int  # Minimum number of transitions to keep in the replay buffer\n",
    "    update_freq::Int  # Number of actions between successive SGD updates\n",
    "    target_update_freq::Int  # Number of actions between target network updates\n",
    "    trajectory_capacity::Int  # Maximum number of trajectories to store in the replay buffer\n",
    "end\n",
    "\n",
    "\"\"\"MisExperimentSettings holds parameters for the configuration of the experiment\"\"\"\n",
    "struct MisExperimentSettings\n",
    "    nbEpisodes::Int\n",
    "    restartPerInstances::Int\n",
    "    evalFreq::Int\n",
    "    nbInstances::Int\n",
    "    nbRandomHeuristics::Int\n",
    "    nbNewVertices::Int\n",
    "    nbInitialVertices::Int\n",
    "    seedEval::Int\n",
    "end\n",
    "\n",
    "agent_config = MisAgentConfig(0.99f0, 64, instance_generator.n, 4, 400, 1, 20, 2000)\n",
    "mis_settings = MisExperimentSettings(300, 1, 10, 10, 1, numNewVertices, numInitialVertices, 123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further configuration\n",
    "\n",
    "Next up, we will define additional configurations for the experiment:\n",
    "- The random seed\n",
    "- Number of steps per episode\n",
    "- The update horizon\n",
    "- The device to use (CPU or GPU)\n",
    "- The evaluation frequency\n",
    "- The steps for the explorer\n",
    "- The parameter initialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1 (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_per_episode = Int(round(mis_settings.nbNewVertices // 2)) + mis_settings.nbInitialVertices\n",
    "update_horizon = Int(round(n_step_per_episode // 2))\n",
    "device = cpu # change if you have a GPU\n",
    "\n",
    "if device == gpu\n",
    "    CUDA.device!(numDevice)\n",
    "end\n",
    "\n",
    "evalFreq = mis_settings.evalFreq\n",
    "step_explorer = Int(floor(mis_settings.nbEpisodes * n_step_per_episode / 2))\n",
    "generator = instance_generator\n",
    "eval_generator = generator\n",
    "\n",
    "rngExp = MersenneTwister(mis_settings.seedEval)\n",
    "init = Flux.glorot_uniform(MersenneTwister(mis_settings.seedEval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The State Representation\n",
    "\n",
    "The state representation is defined in the `HeterogeneousStateRepresentation` class. It is a heterogeneous state representation, which means that it is comprised of multiple state representations. In this example, we will use the `DefaultFeaturization` and the `HeterogeneousTrajectoryState` state representations. The `DefaultFeaturization` state representation is a featurization that is used by default in SeaPearl and allows the user to select the graph features they want. The available features are the following.\n",
    "### Variable Features:\n",
    "- node_number_of_neighbors\n",
    "- variable_initial_domain_size\n",
    "- variable_domain_size\n",
    "- variable_is_bound\n",
    "- variable_is_branchable\n",
    "- variable_is_objective\n",
    "- variable_assigned_value\n",
    "### Constraint Features\n",
    "- node_number_of_neighbors\n",
    "- constraint_activity\n",
    "- nb_involved_constraint_propagation\n",
    "- nb_not_bounded_variable\n",
    "- constraint_type\n",
    "### Value Features\n",
    "- node_number_of_neighbors\n",
    "- values_raw\n",
    "- values_onehot\n",
    "\n",
    "The featurization is used to convert the problem instance to a graph. The `HeterogeneousTrajectoryState` state representation is a state representation that is used to represent the state at a given point in the resolution of the problem. It contains the variable that is branched on, the feature graph at that point and the available values.\n",
    "\n",
    "### Visual representation\n",
    "\n",
    "The following image shows an example of a state representation. Here:\n",
    "- $f_1$ represents features attached to variables ($X_1, X_2, X_3$)\n",
    "- $f_2$ represents features attached to constraints ($C_1, C_2, C_3$)\n",
    "- $f_3$ represents features attached to values (1, 2, 3)\n",
    "\n",
    "<img src=\"img/state-representation.png\" alt=\"State Representation\" style=\"width:700px; height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.HeterogeneousStateRepresentation{SeaPearl.DefaultFeaturization, SeaPearl.HeterogeneousTrajectoryState}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines the features that will be used\n",
    "chosen_features = Dict(\n",
    "    \"node_number_of_neighbors\" => true,\n",
    "    \"constraint_type\" => true,\n",
    "    \"constraint_activity\" => true,\n",
    "    \"nb_not_bounded_variable\" => true,\n",
    "    \"variable_initial_domain_size\" => true,\n",
    "    \"variable_domain_size\" => true,\n",
    "    \"variable_is_objective\" => true,\n",
    "    \"variable_assigned_value\" => true,\n",
    "    \"variable_is_bound\" => true,\n",
    "    \"values_raw\" => true\n",
    ")\n",
    "SR_heterogeneous = SeaPearl.HeterogeneousStateRepresentation{SeaPearl.DefaultFeaturization,SeaPearl.HeterogeneousTrajectoryState}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "In the next cell, we will define the neural network used by the RL agent. We will be using Graph Neural Networks (GNNs) as the learnable architecture. The inputs will be graphs as defined earlier. The features are defined in a Dict and contain elements coming from the problem instance.\n",
    "\n",
    "## The GNN model\n",
    "\n",
    "The model used in this example is drawn from [Marty et al. 2023](https://arxiv.org/abs/2301.01913). The model is defined as a heterogeneous GNN, meaning that every node type uses a specific convolution. The model is comprised of the following elements:\n",
    "\n",
    "### GNN encoder\n",
    "The update equations are given by the following:\n",
    "\\begin{align*}\n",
    "h_{x}^{k+1} &= g\\left(\\theta_{1}^{k}h_{x}^{0} || \\theta_{2}^{k}h_{x}^{k} || \\oplus_{c\\in N_{c}(x)}\\theta_{3}^{k}h_{c}^{k} || \\oplus_{v\\in N_{v}(x)}\\theta_{4}^{k}h_{v}^{k}\\right) \\quad &\\forall x \\in V_1 \\\\\n",
    "h_{c}^{k+1} &= g\\left(\\theta_{5}^{k}h_{c}^{0} || \\theta_{6}^{k}h_{c}^{k} || \\oplus_{x\\in N_{x}(c)}\\theta_{7}^{k}h_{x}^{k}\\right) \\quad &\\forall c \\in V_2 \\\\\n",
    "h_{v}^{k+1} &= g\\left(\\theta_{8}^{k}h_{v}^{0} || \\theta_{9}^{k}h_{v}^{k} || \\oplus_{x\\in N_{x}(v)}\\theta_{10}^{k}h_{x}^{k}\\right) \\quad &\\forall v \\in V_3\n",
    "\\end{align*}\n",
    "\n",
    "- $h_{x}^{k}$, $h_{c}^{k}$, $h_{v}^{k}$ are the feature vectors for nodes of type variable, constraint, and value at layer $k$ respectively.\n",
    "- $h_{x}^{0}$, $h_{c}^{0}$, $h_{v}^{0}$ are the feature vectors for nodes of type variable, constraint, and value from the previous layer. They represent the skip-connections.\n",
    "- $g$ is the leakyReLU activation function which introduces non-linearity into the model.\n",
    "- $\\oplus$ is the mean aggregation function.\n",
    "- $||$ is the concatenation operator.\n",
    "- $\\theta_{i}^{k}$ are weight matrices at layer $k$ which are learned during training.\n",
    "- $N_{c}$, $N_{v}$ and $N_{x}$ are the sets of neighboring nodes for each node.\n",
    "\n",
    "### The action decoder\n",
    "Actions are selected using an epsilon-greedy policy. The Q-values are computed like this:\n",
    "\n",
    "$$\\widehat Q(h_{x}^{K}, h_{v}^{K}) = \\phi_{q}(\\phi_{x}(h_{x}^{K}) \\oplus \\phi_{v}(h_{v}^{K})) \\quad \\forall v \\in V_{x}$$\n",
    "\n",
    "- $h_{x}^{K}$ and $h_{v}^{K}$ are the final node embeddings for the variable and value nodes, respectively, after $K$ iterations in the GNN.\n",
    "- $\\phi_{x}$ and $\\phi_{v}$ are fully connected neural networks that take as input the embeddings of the variable and value nodes, respectively. They further transform these embeddings into an intermediate representation.\n",
    "- $\\oplus$ denotes concatenation of vectors.\n",
    "- $\\phi_{q}$ is another fully connected neural network that takes the concatenated vector as input and outputs a single scalar value, the Q-value.\n",
    "- $V_{x}$ is the subset of value nodes that can be assigned to variable $x$.\n",
    "\n",
    "\n",
    "### Visual Representation\n",
    " \n",
    "Putting it all together, we get the following model:\n",
    "\n",
    "<img src=\"img/High-level-architecture.png\" alt=\"Architecture\" style=\"width:900px; height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_model (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct HeterogeneousModel{A,B}\n",
    "    Inputlayer::A\n",
    "    Middlelayers::Vector{B}\n",
    "end\n",
    "\n",
    "# The size of the input features for each type of node (variable, constraint, value), respectively\n",
    "feature_size = [6, 5, 2]\n",
    "\n",
    "\"\"\"\n",
    "    get_dense_chain(in, mid, out, n_layers, Ïƒ=Flux.identity; init=Flux.glorot_uniform)\n",
    "\n",
    "Create a chain of dense layers for a neural network.\n",
    "\n",
    "# Arguments\n",
    "- `in::Int`: The size of the input layer.\n",
    "- `mid::Int`: The size of the intermediate layers.\n",
    "- `out::Int`: The size of the output layer.\n",
    "- `n_layers::Int`: The number of layers in the chain.\n",
    "- `Ïƒ::Function=Flux.identity`: The activation function to use.\n",
    "- `init::Function=Flux.glorot_uniform`: The initialization method to use.\n",
    "\n",
    "# Returns\n",
    "A `Flux.Chain` object representing the chain of dense layers.\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "julia> get_dense_chain(10, 20, 5, 3)\n",
    "Chain(Dense(10, 20, Ïƒ), Dense(20, 20, Ïƒ), Dense(20, 5))\n",
    "```\n",
    "\"\"\"\n",
    "function get_dense_chain(in, mid, out, n_layers, Ïƒ=Flux.identity; init=Flux.glorot_uniform)\n",
    "    @assert n_layers >= 1\n",
    "    layers = []\n",
    "    if n_layers == 1\n",
    "        push!(layers, Flux.Dense(in, out, init=init))\n",
    "    elseif n_layers == 2\n",
    "        push!(layers, Flux.Dense(in, mid, Ïƒ, init=init))\n",
    "        push!(layers, Flux.Dense(mid, out, init=init))\n",
    "    else\n",
    "        push!(layers, Flux.Dense(in, mid, Ïƒ, init=init))\n",
    "        for i in 2:(n_layers-1)\n",
    "            push!(layers, Flux.Dense(mid, mid, Ïƒ, init=init))\n",
    "        end\n",
    "        push!(layers, Flux.Dense(mid, out, init=init))\n",
    "    end\n",
    "    return Flux.Chain(layers...)\n",
    "end\n",
    "\n",
    "# Builds the SeaPearl HeterogeneousFullFeaturedCPNN model\n",
    "function build_model(; \n",
    "        feature_size,\n",
    "        conv_size=8,\n",
    "        dense_size=16,\n",
    "        output_size=1,\n",
    "        n_layers_graph=3,\n",
    "        n_layers_node=2,\n",
    "        n_layers_output=2,\n",
    "        pool=SeaPearl.meanPooling(),\n",
    "        Ïƒ=Flux.leakyrelu,\n",
    "        init=Flux.glorot_uniform,\n",
    "        device=cpu\n",
    "    )\n",
    "    input_layer = SeaPearl.HeterogeneousGraphConvInit(feature_size, conv_size, Ïƒ, init=init) # input layer\n",
    "    middle_layers = SeaPearl.HeterogeneousGraphConv[] # middle layers\n",
    "    for i in 1:n_layers_graph-1\n",
    "        push!(middle_layers, SeaPearl.HeterogeneousGraphConv(conv_size => conv_size, feature_size, Ïƒ, pool=pool, init=init))\n",
    "    end\n",
    "    output_layer = SeaPearl.HeterogeneousGraphConv(conv_size => output_size, feature_size, Ïƒ, pool=pool, init=init) # output layer\n",
    "    dense_layers = get_dense_chain(conv_size, dense_size, dense_size, n_layers_node, Ïƒ, init=init) # dense layers\n",
    "    # Define the final output layer\n",
    "    final_output_layer = get_dense_chain(2 * dense_size, dense_size, output_size, n_layers_output, Ïƒ, init=init)\n",
    "\n",
    "    # Build the model\n",
    "    model = SeaPearl.HeterogeneousFullFeaturedCPNN(\n",
    "        HeterogeneousModel(input_layer, middle_layers),\n",
    "        dense_layers,\n",
    "        Flux.Chain(),\n",
    "        final_output_layer\n",
    "    ) |> device\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent, Replay Buffer and Exploration Policy\n",
    "\n",
    "We now have:\n",
    "- The reward function\n",
    "- An instance generator\n",
    "- A GNN\n",
    "- And all the settings we need!\n",
    "\n",
    "We now need to define:\n",
    "- The way we will store trajectories will be stored (in a circular buffer)\n",
    "- The exploration policy (eps-greedy)\n",
    "- The agent (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_epsilon_greedy_explorer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    get_heterogeneous_slart_trajectory(; capacity, n_actions)\n",
    "\n",
    "Create a circular buffer for storing trajectories in the context of reinforcement learning where not all actions are legal. \n",
    "SLART stands for State, Legal Actions, Reward, Terminal.\n",
    "\n",
    "# Arguments\n",
    "- `capacity::Int`: The maximum number of trajectories that can be stored in the buffer.\n",
    "- `n_actions::Int`: The number of possible actions that can be taken at each time step.\n",
    "\n",
    "# Returns\n",
    "A `CircularArraySLARTTrajectory` object with the specified capacity and legal actions mask, and an empty state buffer.\n",
    "\"\"\"\n",
    "function get_heterogeneous_slart_trajectory(; capacity, n_actions)\n",
    "    return RL.CircularArraySLARTTrajectory(\n",
    "        capacity=capacity,\n",
    "        state=SeaPearl.HeterogeneousTrajectoryState[] => (),\n",
    "        legal_actions_mask=Vector{Bool} => (n_actions,),\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"Builds the DQN agent for the heterogeneous model\"\"\"\n",
    "function get_heterogeneous_agent(; get_explorer, batch_size=16, update_horizon, min_replay_history, update_freq=1, target_update_freq=200, Î³=0.999f0, get_heterogeneous_trajectory, get_heterogeneous_nn)\n",
    "    return RL.Agent(\n",
    "        policy=RL.QBasedPolicy(\n",
    "            learner=get_heterogeneous_learner(batch_size, update_horizon, min_replay_history, update_freq, target_update_freq, get_heterogeneous_nn, Î³),\n",
    "            explorer=get_explorer(),\n",
    "        ),\n",
    "        trajectory=get_heterogeneous_trajectory()\n",
    "    )\n",
    "end\n",
    "\n",
    "function get_heterogeneous_learner(batch_size, update_horizon, min_replay_history, update_freq, target_update_freq, get_heterogeneous_nn, Î³)\n",
    "    return RL.DQNLearner(\n",
    "        approximator=RL.NeuralNetworkApproximator(\n",
    "            model=get_heterogeneous_nn(),\n",
    "            optimizer=ADAM()\n",
    "        ),\n",
    "        target_approximator=RL.NeuralNetworkApproximator(\n",
    "            model=get_heterogeneous_nn(),\n",
    "            optimizer=ADAM()\n",
    "        ),\n",
    "        loss_func=Flux.Losses.huber_loss,\n",
    "        batch_size=batch_size,\n",
    "        update_horizon=update_horizon,\n",
    "        min_replay_history=min_replay_history,\n",
    "        update_freq=update_freq,\n",
    "        target_update_freq=target_update_freq,\n",
    "        Î³=Î³\n",
    "    )\n",
    "end\n",
    "\n",
    "Flux.@functor HeterogeneousModel # To allow automatic differentiation\n",
    "\"\"\"\n",
    "function Flux.functor(::Type{<:HeterogeneousModel}, m)\n",
    "    return (m.Inputlayer, m.Middlelayers), ls -> HeterogeneousModel(ls[1], ls[2])\n",
    "end\n",
    "\"\"\"\n",
    "function (m::HeterogeneousModel)(fg)\n",
    "    original_fg = deepcopy(fg)\n",
    "    out = m.Inputlayer(fg)\n",
    "    for layer in m.Middlelayers\n",
    "        out = layer(out, original_fg)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    get_epsilon_greedy_explorer(decay_steps, Ïµ_stable; rng=nothing)\n",
    "\n",
    "Create an epsilon-greedy explorer for use in reinforcement learning.\n",
    "\n",
    "# Arguments\n",
    "- `decay_steps::Int`: The number of steps over which to decay the exploration rate.\n",
    "- `Ïµ_stable::Real`: The minimum exploration rate to use after decay.\n",
    "- `rng::AbstractRNG`: (optional) A random number generator to use for sampling actions.\n",
    "\n",
    "# Returns\n",
    "An `EpsilonGreedyExplorer` object with the specified exploration rate decay and random number generator.\n",
    "\"\"\"\n",
    "function get_epsilon_greedy_explorer(decay_steps, Ïµ_stable; rng=nothing)\n",
    "    if isnothing(rng)\n",
    "        return RL.EpsilonGreedyExplorer(\n",
    "            Ïµ_stable=Ïµ_stable,\n",
    "            kind=:exp,\n",
    "            decay_steps=decay_steps,\n",
    "            step=1\n",
    "        )\n",
    "    else\n",
    "        return RL.EpsilonGreedyExplorer(\n",
    "            Ïµ_stable=Ïµ_stable,\n",
    "            kind=:exp,\n",
    "            decay_steps=decay_steps,\n",
    "            step=1,\n",
    "            rng=rng\n",
    "        )\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent definition\n",
    "\n",
    "The agent and the related learned heuristic are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.SimpleLearnedHeuristic{SeaPearl.HeterogeneousStateRepresentation{SeaPearl.DefaultFeaturization, SeaPearl.HeterogeneousTrajectoryState}, SeaPearl.GeneralReward, SeaPearl.FixedOutput}(typename(Agent)\n",
       "â”œâ”€ policy => typename(QBasedPolicy)\n",
       "â”‚  â”œâ”€ learner => typename(DQNLearner)\n",
       "â”‚  â”‚  â”œâ”€ approximator => typename(NeuralNetworkApproximator)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ model => typename(SeaPearl.HeterogeneousFullFeaturedCPNN)\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ graphChain => typename(HeterogeneousModel)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Inputlayer => typename(SeaPearl.HeterogeneousGraphConvInit)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ weightsvar => 8Ã—6 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ weightscon => 8Ã—5 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ weightsval => 8Ã—2 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ biasvar => 8-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ biascon => 8-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ biasval => 8-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Middlelayers => 2-element Vector{SeaPearl.HeterogeneousGraphConv{Matrix{Float32}, Vector{Float32}, SeaPearl.meanPooling}}\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ varChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—8 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ valChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—8 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ globalChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚  â””â”€ outputChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚     â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚        â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚     â”œâ”€ weight => 16Ã—32 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚        â””â”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚           â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚              â”œâ”€ weight => 1Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚              â”œâ”€ bias => 1-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚              â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â””â”€ optimizer => typename(ADAM)\n",
       "â”‚  â”‚  â”‚     â”œâ”€ eta => 0.001\n",
       "â”‚  â”‚  â”‚     â”œâ”€ beta\n",
       "â”‚  â”‚  â”‚     â”‚  â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚     â”‚  â”‚  â””â”€ 0.9\n",
       "â”‚  â”‚  â”‚     â”‚  â””â”€ 2\n",
       "â”‚  â”‚  â”‚     â”‚     â””â”€ 0.999\n",
       "â”‚  â”‚  â”‚     â”œâ”€ epsilon => 1.0e-8\n",
       "â”‚  â”‚  â”‚     â””â”€ state => typename(IdDict)\n",
       "â”‚  â”‚  â”œâ”€ target_approximator => typename(NeuralNetworkApproximator)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ model => typename(SeaPearl.HeterogeneousFullFeaturedCPNN)\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ graphChain => typename(HeterogeneousModel)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Inputlayer => typename(SeaPearl.HeterogeneousGraphConvInit)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ weightsvar => 8Ã—6 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ weightscon => 8Ã—5 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ weightsval => 8Ã—2 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ biasvar => 8-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ biascon => 8-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ biasval => 8-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Middlelayers => 2-element Vector{SeaPearl.HeterogeneousGraphConv{Matrix{Float32}, Vector{Float32}, SeaPearl.meanPooling}}\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ varChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—8 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ valChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—8 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ weight => 16Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â”‚  â”œâ”€ globalChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚  â””â”€ outputChain => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚     â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚        â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚     â”œâ”€ weight => 16Ã—32 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚     â”œâ”€ bias => 16-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚        â”‚     â””â”€ Ïƒ => typename(typeof(leakyrelu))\n",
       "â”‚  â”‚  â”‚  â”‚        â””â”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚           â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚              â”œâ”€ weight => 1Ã—16 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚              â”œâ”€ bias => 1-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚              â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â””â”€ optimizer => typename(ADAM)\n",
       "â”‚  â”‚  â”‚     â”œâ”€ eta => 0.001\n",
       "â”‚  â”‚  â”‚     â”œâ”€ beta\n",
       "â”‚  â”‚  â”‚     â”‚  â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚     â”‚  â”‚  â””â”€ 0.9\n",
       "â”‚  â”‚  â”‚     â”‚  â””â”€ 2\n",
       "â”‚  â”‚  â”‚     â”‚     â””â”€ 0.999\n",
       "â”‚  â”‚  â”‚     â”œâ”€ epsilon => 1.0e-8\n",
       "â”‚  â”‚  â”‚     â””â”€ state => typename(IdDict)\n",
       "â”‚  â”‚  â”œâ”€ loss_func => typename(typeof(Flux.Losses.huber_loss))\n",
       "â”‚  â”‚  â”œâ”€ min_replay_history => 56\n",
       "â”‚  â”‚  â”œâ”€ update_freq => 1\n",
       "â”‚  â”‚  â”œâ”€ update_step => 0\n",
       "â”‚  â”‚  â”œâ”€ target_update_freq => 20\n",
       "â”‚  â”‚  â”œâ”€ sampler => typename(NStepBatchSampler)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ Î³ => 0.99\n",
       "â”‚  â”‚  â”‚  â”œâ”€ n => 4\n",
       "â”‚  â”‚  â”‚  â”œâ”€ batch_size => 64\n",
       "â”‚  â”‚  â”‚  â”œâ”€ stack_size => typename(Nothing)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ rng => typename(Random._GLOBAL_RNG)\n",
       "â”‚  â”‚  â”‚  â””â”€ cache => typename(Nothing)\n",
       "â”‚  â”‚  â”œâ”€ rng => typename(Random._GLOBAL_RNG)\n",
       "â”‚  â”‚  â”œâ”€ loss => 0.0\n",
       "â”‚  â”‚  â””â”€ is_enable_double_DQN => true\n",
       "â”‚  â””â”€ explorer => typename(EpsilonGreedyExplorer)\n",
       "â”‚     â”œâ”€ Ïµ_stable => 0.01\n",
       "â”‚     â”œâ”€ Ïµ_init => 1.0\n",
       "â”‚     â”œâ”€ warmup_steps => 0\n",
       "â”‚     â”œâ”€ decay_steps => 1050\n",
       "â”‚     â”œâ”€ step => 1\n",
       "â”‚     â”œâ”€ rng => typename(MersenneTwister)\n",
       "â”‚     â””â”€ is_training => true\n",
       "â””â”€ trajectory => typename(Trajectory)\n",
       "   â””â”€ traces => typename(NamedTuple)\n",
       "      â”œâ”€ state => 0-element CircularArrayBuffers.CircularVectorBuffer{SeaPearl.HeterogeneousTrajectoryState, Vector{SeaPearl.HeterogeneousTrajectoryState}}\n",
       "      â”œâ”€ legal_actions_mask => 2Ã—0 CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}\n",
       "      â”œâ”€ action => 0-element CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}\n",
       "      â”œâ”€ reward => 0-element CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}\n",
       "      â””â”€ terminal => 0-element CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}\n",
       ", nothing, nothing, nothing, nothing, nothing, nothing, false, true, Dict{String, Bool}(\"variable_initial_domain_size\" => 1, \"values_raw\" => 1, \"constraint_type\" => 1, \"variable_is_bound\" => 1, \"nb_not_bounded_variable\" => 1, \"variable_assigned_value\" => 1, \"node_number_of_neighbors\" => 1, \"variable_is_objective\" => 1, \"variable_domain_size\" => 1, \"constraint_activity\" => 1â€¦))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = SeaPearl.meanPooling()\n",
    "\n",
    "agent = get_heterogeneous_agent(;\n",
    "    get_heterogeneous_trajectory=() -> get_heterogeneous_slart_trajectory(capacity=agent_config.trajectory_capacity, n_actions=2),\n",
    "    get_explorer=() -> get_epsilon_greedy_explorer(step_explorer, 0.01; rng=rngExp),\n",
    "    batch_size=agent_config.batch_size,\n",
    "    update_horizon=update_horizon,\n",
    "    min_replay_history=Int(round(16 * n_step_per_episode // 2)),\n",
    "    update_freq=agent_config.update_freq,\n",
    "    target_update_freq=agent_config.target_update_freq,\n",
    "    get_heterogeneous_nn=() -> build_model(\n",
    "        feature_size=feature_size,\n",
    "        conv_size=8,\n",
    "        dense_size=16,\n",
    "        output_size=1,\n",
    "        n_layers_graph=3,\n",
    "        n_layers_node=3,\n",
    "        n_layers_output=2,\n",
    "        pool=pool,\n",
    "        Ïƒ=NNlib.leakyrelu,\n",
    "        init=init,\n",
    "        device=device\n",
    "    ),\n",
    "    Î³=0.99f0\n",
    ")\n",
    "\n",
    "learned_heuristic = SeaPearl.SimpleLearnedHeuristic{SR_heterogeneous,reward,SeaPearl.FixedOutput}(agent; chosen_features=chosen_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up comparisons and running the experiment\n",
    "\n",
    "We now have everything we need to run the experiment. We will run the experiment for 1000 episodes and compare the performance of the agent with\n",
    "- A random agent \n",
    "- Heuristic that always selects the max value available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.MinDomainVariableSelection{false}()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectMax(x::SeaPearl.IntVar; cpmodel=nothing) = SeaPearl.maximum(x.domain)\n",
    "heuristic_max = SeaPearl.BasicHeuristic(selectMax)\n",
    "\n",
    "function select_random_value(x::SeaPearl.IntVar; cpmodel=nothing)\n",
    "    selected_number = rand(1:length(x.domain))\n",
    "    i = 1\n",
    "    for value in x.domain\n",
    "        if i == selected_number\n",
    "            return value\n",
    "        end\n",
    "        i += 1\n",
    "    end\n",
    "    @assert false \"This should not happen\"\n",
    "end\n",
    "\n",
    "randomHeuristics = []\n",
    "for i in 1:mis_settings.nbRandomHeuristics\n",
    "    push!(randomHeuristics, SeaPearl.BasicHeuristic(select_random_value))\n",
    "end\n",
    "\n",
    "valueSelectionArray = [learned_heuristic, heuristic_max]\n",
    "append!(valueSelectionArray, randomHeuristics)\n",
    "variableSelection = SeaPearl.MinDomainVariableSelection{false}()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the problem\n",
    "\n",
    "Let's finally build a function that will help solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_mis (generic function with 3 methods)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function learning_mis(\n",
    "    agent::RL.Agent,\n",
    "    agent_config::MisAgentConfig,\n",
    "    mis_settings::MisExperimentSettings,\n",
    "    instance_generator::SeaPearl.AbstractModelGenerator,\n",
    "    save_experiment_parameters::Bool=false,\n",
    "    save_model::Bool=false\n",
    ")\n",
    "    # To save the experiment parameters\n",
    "    if save_experiment_parameters\n",
    "        experiment_time = now()\n",
    "        dir = mkdir(string(\"exp_\", Base.replace(\"$(round(experiment_time, Dates.Second(3)))\", \":\" => \"-\")))\n",
    "        experiment_parameters = get_experiment_parameters(agent, agent_config, mis_settings)\n",
    "        open(dir * \"/params.json\", \"w\") do file\n",
    "            JSON.print(file, experiment_parameters)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    metricsArray, eval_metricsArray = SeaPearl.train!(\n",
    "        valueSelectionArray=valueSelectionArray,\n",
    "        generator=instance_generator,\n",
    "        nbEpisodes=mis_settings.nbEpisodes,\n",
    "        strategy=SeaPearl.DFSearch(),\n",
    "        variableHeuristic=variableSelection,\n",
    "        out_solver=true,\n",
    "        verbose=false,\n",
    "        evaluator=SeaPearl.SameInstancesEvaluator(valueSelectionArray, instance_generator; evalFreq=mis_settings.evalFreq, nbInstances=mis_settings.nbInstances),\n",
    "        restartPerInstances=mis_settings.restartPerInstances\n",
    "    )\n",
    "    # To save the model\n",
    "    if save_model\n",
    "        dir = pwd()\n",
    "        model = agent.policy.learner.approximator\n",
    "        @save dir * \"/model_mis\" * string(instance_generator.n) * \".bson\" model\n",
    "    end\n",
    "\n",
    "    return metricsArray, eval_metricsArray\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsArray, eval_metricsArray = learning_mis(agent, agent_config, mis_settings, instance_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results\n",
    "\n",
    "Now that we have a small trained model, let's look at the results. First, we will have a look at the performance of the learned heuristic vs random and select max heuristic over the course of the training. Then, we will compare their performance on unseen instances of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility functions fetch and organize training metrics + plotting & benchmarking\n",
    "include(\"../../../learning_cp/utils/save_metrics.jl\")\n",
    "include(\"../../../learning_cp/utils/plot_metrics.jl\")\n",
    "include(\"../../../learning_cp/utils/benchmark.jl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = get_metrics_dataframe(eval_metricsArray)\n",
    "plot_first_solution(training_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on unseen instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, Plots\n",
    "\n",
    "validation_generator = SeaPearl.MaximumIndependentSetGenerator(8, 3)\n",
    "\n",
    "num_instances = 20 # Number of instances to evaluate on\n",
    "node_budget = 10000 # Budget of visited nodes\n",
    "take_objective = false # Set it to true if we have to branch on the object ive variable\n",
    "eval_strategy = SeaPearl.DFSearch()\n",
    "include_dfs = true # Set it to true if you want to evaluate with DFS in addition to ILDS\n",
    "basicHeuristics = Dict()\n",
    "num_random_heuristics = 2\n",
    "\n",
    "for (i, random_heuristic) in enumerate(randomHeuristics)\n",
    "    push!(basicHeuristics, \"random\"*string(i) => random_heuristic)\n",
    "end\n",
    "\n",
    "push!(basicHeuristics, \"max\" => heuristic_max)\n",
    "evaluation_df = benchmark(\n",
    "    models=[agent.policy.learner.approximator], \n",
    "    evaluation_folder=pwd(),\n",
    "    num_instances=num_instances, \n",
    "    chosen_features=chosen_features,\n",
    "    take_objective=take_objective,\n",
    "    generator=validation_generator,\n",
    "    basicHeuristics=basicHeuristics,\n",
    "    save_experiment_metrics=false,\n",
    "    include_dfs=include_dfs, \n",
    "    budget=node_budget,\n",
    "    ILDS=eval_strategy\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing heuristics across 20 instances\n",
    "\n",
    "We will now compare the performance of the learned heuristic with the random and select max heuristics on 20 unseen instances of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip940\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip941\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M212.459 1423.18 L2352.76 1423.18 L2352.76 47.2441 L212.459 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip942\">\n",
       "    <rect x=\"212\" y=\"47\" width=\"2141\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"698.117,1423.18 698.117,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1229.47,1423.18 1229.47,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1760.83,1423.18 1760.83,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2292.18,1423.18 2292.18,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"212.459,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"698.117,1423.18 698.117,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1229.47,1423.18 1229.47,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1760.83,1423.18 1760.83,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2292.18,1423.18 2292.18,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M688.395 1451.02 L706.751 1451.02 L706.751 1454.96 L692.677 1454.96 L692.677 1463.43 Q693.696 1463.08 694.714 1462.92 Q695.733 1462.73 696.751 1462.73 Q702.538 1462.73 705.918 1465.9 Q709.298 1469.08 709.298 1474.49 Q709.298 1480.07 705.825 1483.17 Q702.353 1486.25 696.034 1486.25 Q693.858 1486.25 691.589 1485.88 Q689.344 1485.51 686.937 1484.77 L686.937 1480.07 Q689.02 1481.2 691.242 1481.76 Q693.464 1482.32 695.941 1482.32 Q699.946 1482.32 702.284 1480.21 Q704.622 1478.1 704.622 1474.49 Q704.622 1470.88 702.284 1468.77 Q699.946 1466.67 695.941 1466.67 Q694.066 1466.67 692.191 1467.08 Q690.339 1467.5 688.395 1468.38 L688.395 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1204.16 1481.64 L1211.8 1481.64 L1211.8 1455.28 L1203.49 1456.95 L1203.49 1452.69 L1211.75 1451.02 L1216.43 1451.02 L1216.43 1481.64 L1224.07 1481.64 L1224.07 1485.58 L1204.16 1485.58 L1204.16 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1243.51 1454.1 Q1239.9 1454.1 1238.07 1457.66 Q1236.27 1461.2 1236.27 1468.33 Q1236.27 1475.44 1238.07 1479.01 Q1239.9 1482.55 1243.51 1482.55 Q1247.15 1482.55 1248.95 1479.01 Q1250.78 1475.44 1250.78 1468.33 Q1250.78 1461.2 1248.95 1457.66 Q1247.15 1454.1 1243.51 1454.1 M1243.51 1450.39 Q1249.32 1450.39 1252.38 1455 Q1255.46 1459.58 1255.46 1468.33 Q1255.46 1477.06 1252.38 1481.67 Q1249.32 1486.25 1243.51 1486.25 Q1237.7 1486.25 1234.62 1481.67 Q1231.57 1477.06 1231.57 1468.33 Q1231.57 1459.58 1234.62 1455 Q1237.7 1450.39 1243.51 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1736.01 1481.64 L1743.65 1481.64 L1743.65 1455.28 L1735.34 1456.95 L1735.34 1452.69 L1743.6 1451.02 L1748.28 1451.02 L1748.28 1481.64 L1755.92 1481.64 L1755.92 1485.58 L1736.01 1485.58 L1736.01 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1765.41 1451.02 L1783.77 1451.02 L1783.77 1454.96 L1769.69 1454.96 L1769.69 1463.43 Q1770.71 1463.08 1771.73 1462.92 Q1772.75 1462.73 1773.77 1462.73 Q1779.55 1462.73 1782.93 1465.9 Q1786.31 1469.08 1786.31 1474.49 Q1786.31 1480.07 1782.84 1483.17 Q1779.37 1486.25 1773.05 1486.25 Q1770.87 1486.25 1768.6 1485.88 Q1766.36 1485.51 1763.95 1484.77 L1763.95 1480.07 Q1766.03 1481.2 1768.26 1481.76 Q1770.48 1482.32 1772.96 1482.32 Q1776.96 1482.32 1779.3 1480.21 Q1781.64 1478.1 1781.64 1474.49 Q1781.64 1470.88 1779.3 1468.77 Q1776.96 1466.67 1772.96 1466.67 Q1771.08 1466.67 1769.21 1467.08 Q1767.35 1467.5 1765.41 1468.38 L1765.41 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2270.95 1481.64 L2287.27 1481.64 L2287.27 1485.58 L2265.33 1485.58 L2265.33 1481.64 Q2267.99 1478.89 2272.58 1474.26 Q2277.18 1469.61 2278.36 1468.27 Q2280.61 1465.74 2281.49 1464.01 Q2282.39 1462.25 2282.39 1460.56 Q2282.39 1457.8 2280.45 1456.07 Q2278.52 1454.33 2275.42 1454.33 Q2273.22 1454.33 2270.77 1455.09 Q2268.34 1455.86 2265.56 1457.41 L2265.56 1452.69 Q2268.39 1451.55 2270.84 1450.97 Q2273.29 1450.39 2275.33 1450.39 Q2280.7 1450.39 2283.89 1453.08 Q2287.09 1455.77 2287.09 1460.26 Q2287.09 1462.39 2286.28 1464.31 Q2285.49 1466.2 2283.39 1468.8 Q2282.81 1469.47 2279.7 1472.69 Q2276.6 1475.88 2270.95 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2307.09 1454.1 Q2303.48 1454.1 2301.65 1457.66 Q2299.84 1461.2 2299.84 1468.33 Q2299.84 1475.44 2301.65 1479.01 Q2303.48 1482.55 2307.09 1482.55 Q2310.72 1482.55 2312.53 1479.01 Q2314.36 1475.44 2314.36 1468.33 Q2314.36 1461.2 2312.53 1457.66 Q2310.72 1454.1 2307.09 1454.1 M2307.09 1450.39 Q2312.9 1450.39 2315.95 1455 Q2319.03 1459.58 2319.03 1468.33 Q2319.03 1477.06 2315.95 1481.67 Q2312.9 1486.25 2307.09 1486.25 Q2301.28 1486.25 2298.2 1481.67 Q2295.14 1477.06 2295.14 1468.33 Q2295.14 1459.58 2298.2 1455 Q2301.28 1450.39 2307.09 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1005.89 1520.52 L1012.32 1520.52 L1012.32 1568.04 L1005.89 1568.04 L1005.89 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1054.49 1546.53 L1054.49 1568.04 L1048.64 1568.04 L1048.64 1546.72 Q1048.64 1541.66 1046.66 1539.14 Q1044.69 1536.63 1040.74 1536.63 Q1036 1536.63 1033.26 1539.65 Q1030.53 1542.68 1030.53 1547.9 L1030.53 1568.04 L1024.64 1568.04 L1024.64 1532.4 L1030.53 1532.4 L1030.53 1537.93 Q1032.63 1534.72 1035.46 1533.13 Q1038.32 1531.54 1042.05 1531.54 Q1048.19 1531.54 1051.34 1535.36 Q1054.49 1539.14 1054.49 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1088.9 1533.45 L1088.9 1538.98 Q1086.42 1537.71 1083.74 1537.07 Q1081.07 1536.44 1078.2 1536.44 Q1073.84 1536.44 1071.65 1537.77 Q1069.48 1539.11 1069.48 1541.79 Q1069.48 1543.82 1071.04 1545 Q1072.6 1546.15 1077.31 1547.2 L1079.32 1547.64 Q1085.56 1548.98 1088.17 1551.43 Q1090.81 1553.85 1090.81 1558.21 Q1090.81 1563.17 1086.86 1566.07 Q1082.95 1568.97 1076.07 1568.97 Q1073.21 1568.97 1070.09 1568.39 Q1067 1567.85 1063.56 1566.74 L1063.56 1560.69 Q1066.81 1562.38 1069.96 1563.24 Q1073.11 1564.07 1076.2 1564.07 Q1080.34 1564.07 1082.57 1562.66 Q1084.79 1561.23 1084.79 1558.65 Q1084.79 1556.27 1083.17 1554.99 Q1081.58 1553.72 1076.14 1552.54 L1074.1 1552.07 Q1068.66 1550.92 1066.24 1548.56 Q1063.82 1546.18 1063.82 1542.04 Q1063.82 1537.01 1067.38 1534.27 Q1070.95 1531.54 1077.5 1531.54 Q1080.75 1531.54 1083.62 1532.01 Q1086.48 1532.49 1088.9 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1105.93 1522.27 L1105.93 1532.4 L1117.99 1532.4 L1117.99 1536.95 L1105.93 1536.95 L1105.93 1556.3 Q1105.93 1560.66 1107.1 1561.9 Q1108.31 1563.14 1111.97 1563.14 L1117.99 1563.14 L1117.99 1568.04 L1111.97 1568.04 Q1105.2 1568.04 1102.62 1565.53 Q1100.04 1562.98 1100.04 1556.3 L1100.04 1536.95 L1095.74 1536.95 L1095.74 1532.4 L1100.04 1532.4 L1100.04 1522.27 L1105.93 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1141.89 1550.12 Q1134.8 1550.12 1132.06 1551.75 Q1129.32 1553.37 1129.32 1557.29 Q1129.32 1560.4 1131.36 1562.25 Q1133.43 1564.07 1136.96 1564.07 Q1141.83 1564.07 1144.76 1560.63 Q1147.72 1557.16 1147.72 1551.43 L1147.72 1550.12 L1141.89 1550.12 M1153.57 1547.71 L1153.57 1568.04 L1147.72 1568.04 L1147.72 1562.63 Q1145.71 1565.88 1142.72 1567.44 Q1139.73 1568.97 1135.4 1568.97 Q1129.93 1568.97 1126.68 1565.91 Q1123.46 1562.82 1123.46 1557.67 Q1123.46 1551.65 1127.48 1548.6 Q1131.52 1545.54 1139.51 1545.54 L1147.72 1545.54 L1147.72 1544.97 Q1147.72 1540.93 1145.04 1538.73 Q1142.4 1536.5 1137.6 1536.5 Q1134.54 1536.5 1131.64 1537.23 Q1128.75 1537.97 1126.07 1539.43 L1126.07 1534.02 Q1129.29 1532.78 1132.31 1532.17 Q1135.34 1531.54 1138.2 1531.54 Q1145.94 1531.54 1149.76 1535.55 Q1153.57 1539.56 1153.57 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1195.27 1546.53 L1195.27 1568.04 L1189.41 1568.04 L1189.41 1546.72 Q1189.41 1541.66 1187.44 1539.14 Q1185.47 1536.63 1181.52 1536.63 Q1176.78 1536.63 1174.04 1539.65 Q1171.3 1542.68 1171.3 1547.9 L1171.3 1568.04 L1165.41 1568.04 L1165.41 1532.4 L1171.3 1532.4 L1171.3 1537.93 Q1173.4 1534.72 1176.24 1533.13 Q1179.1 1531.54 1182.82 1531.54 Q1188.97 1531.54 1192.12 1535.36 Q1195.27 1539.14 1195.27 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1232.6 1533.76 L1232.6 1539.24 Q1230.12 1537.87 1227.61 1537.2 Q1225.13 1536.5 1222.58 1536.5 Q1216.88 1536.5 1213.73 1540.13 Q1210.58 1543.73 1210.58 1550.25 Q1210.58 1556.78 1213.73 1560.4 Q1216.88 1564 1222.58 1564 Q1225.13 1564 1227.61 1563.33 Q1230.12 1562.63 1232.6 1561.26 L1232.6 1566.68 Q1230.15 1567.82 1227.51 1568.39 Q1224.9 1568.97 1221.94 1568.97 Q1213.89 1568.97 1209.15 1563.91 Q1204.4 1558.85 1204.4 1550.25 Q1204.4 1541.53 1209.18 1536.53 Q1213.99 1531.54 1222.32 1531.54 Q1225.03 1531.54 1227.61 1532.11 Q1230.19 1532.65 1232.6 1533.76 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1273.28 1548.76 L1273.28 1551.62 L1246.35 1551.62 Q1246.74 1557.67 1249.98 1560.85 Q1253.26 1564 1259.09 1564 Q1262.46 1564 1265.61 1563.17 Q1268.79 1562.35 1271.91 1560.69 L1271.91 1566.23 Q1268.76 1567.57 1265.45 1568.27 Q1262.14 1568.97 1258.74 1568.97 Q1250.21 1568.97 1245.21 1564 Q1240.24 1559.04 1240.24 1550.57 Q1240.24 1541.82 1244.95 1536.69 Q1249.7 1531.54 1257.72 1531.54 Q1264.91 1531.54 1269.08 1536.18 Q1273.28 1540.8 1273.28 1548.76 M1267.43 1547.04 Q1267.36 1542.23 1264.72 1539.37 Q1262.11 1536.5 1257.78 1536.5 Q1252.88 1536.5 1249.92 1539.27 Q1246.99 1542.04 1246.55 1547.07 L1267.43 1547.04 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1303.87 1520.52 L1312.53 1520.52 L1333.6 1560.28 L1333.6 1520.52 L1339.84 1520.52 L1339.84 1568.04 L1331.18 1568.04 L1310.11 1528.29 L1310.11 1568.04 L1303.87 1568.04 L1303.87 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1351.77 1553.98 L1351.77 1532.4 L1357.63 1532.4 L1357.63 1553.75 Q1357.63 1558.81 1359.6 1561.36 Q1361.57 1563.87 1365.52 1563.87 Q1370.26 1563.87 1373 1560.85 Q1375.77 1557.83 1375.77 1552.61 L1375.77 1532.4 L1381.63 1532.4 L1381.63 1568.04 L1375.77 1568.04 L1375.77 1562.57 Q1373.64 1565.82 1370.8 1567.41 Q1368 1568.97 1364.28 1568.97 Q1358.14 1568.97 1354.95 1565.15 Q1351.77 1561.33 1351.77 1553.98 M1366.51 1531.54 L1366.51 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1421.44 1539.24 Q1423.64 1535.29 1426.7 1533.41 Q1429.75 1531.54 1433.89 1531.54 Q1439.46 1531.54 1442.48 1535.45 Q1445.51 1539.33 1445.51 1546.53 L1445.51 1568.04 L1439.62 1568.04 L1439.62 1546.72 Q1439.62 1541.59 1437.8 1539.11 Q1435.99 1536.63 1432.27 1536.63 Q1427.71 1536.63 1425.07 1539.65 Q1422.43 1542.68 1422.43 1547.9 L1422.43 1568.04 L1416.54 1568.04 L1416.54 1546.72 Q1416.54 1541.56 1414.73 1539.11 Q1412.91 1536.63 1409.13 1536.63 Q1404.64 1536.63 1402 1539.68 Q1399.35 1542.71 1399.35 1547.9 L1399.35 1568.04 L1393.47 1568.04 L1393.47 1532.4 L1399.35 1532.4 L1399.35 1537.93 Q1401.36 1534.66 1404.16 1533.1 Q1406.96 1531.54 1410.81 1531.54 Q1414.7 1531.54 1417.4 1533.51 Q1420.14 1535.48 1421.44 1539.24 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1482.78 1550.25 Q1482.78 1543.79 1480.1 1540.13 Q1477.46 1536.44 1472.81 1536.44 Q1468.17 1536.44 1465.49 1540.13 Q1462.85 1543.79 1462.85 1550.25 Q1462.85 1556.71 1465.49 1560.4 Q1468.17 1564.07 1472.81 1564.07 Q1477.46 1564.07 1480.1 1560.4 Q1482.78 1556.71 1482.78 1550.25 M1462.85 1537.81 Q1464.7 1534.62 1467.5 1533.1 Q1470.33 1531.54 1474.25 1531.54 Q1480.74 1531.54 1484.78 1536.69 Q1488.86 1541.85 1488.86 1550.25 Q1488.86 1558.65 1484.78 1563.81 Q1480.74 1568.97 1474.25 1568.97 Q1470.33 1568.97 1467.5 1567.44 Q1464.7 1565.88 1462.85 1562.7 L1462.85 1568.04 L1456.96 1568.04 L1456.96 1518.52 L1462.85 1518.52 L1462.85 1537.81 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1529.06 1548.76 L1529.06 1551.62 L1502.13 1551.62 Q1502.51 1557.67 1505.76 1560.85 Q1509.04 1564 1514.86 1564 Q1518.23 1564 1521.39 1563.17 Q1524.57 1562.35 1527.69 1560.69 L1527.69 1566.23 Q1524.54 1567.57 1521.23 1568.27 Q1517.92 1568.97 1514.51 1568.97 Q1505.98 1568.97 1500.98 1564 Q1496.02 1559.04 1496.02 1550.57 Q1496.02 1541.82 1500.73 1536.69 Q1505.47 1531.54 1513.49 1531.54 Q1520.68 1531.54 1524.85 1536.18 Q1529.06 1540.8 1529.06 1548.76 M1523.2 1547.04 Q1523.14 1542.23 1520.49 1539.37 Q1517.88 1536.5 1513.56 1536.5 Q1508.65 1536.5 1505.69 1539.27 Q1502.77 1542.04 1502.32 1547.07 L1523.2 1547.04 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1559.32 1537.87 Q1558.34 1537.3 1557.16 1537.04 Q1556.01 1536.76 1554.61 1536.76 Q1549.65 1536.76 1546.98 1540 Q1544.33 1543.22 1544.33 1549.27 L1544.33 1568.04 L1538.45 1568.04 L1538.45 1532.4 L1544.33 1532.4 L1544.33 1537.93 Q1546.18 1534.69 1549.14 1533.13 Q1552.1 1531.54 1556.33 1531.54 Q1556.94 1531.54 1557.67 1531.63 Q1558.4 1531.7 1559.29 1531.85 L1559.32 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"212.459,1384.24 2352.76,1384.24 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"212.459,951.554 2352.76,951.554 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"212.459,518.87 2352.76,518.87 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"212.459,86.1857 2352.76,86.1857 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"212.459,1423.18 212.459,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"212.459,1384.24 231.357,1384.24 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"212.459,951.554 231.357,951.554 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"212.459,518.87 231.357,518.87 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"212.459,86.1857 231.357,86.1857 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M114.26 1384.69 L143.936 1384.69 L143.936 1388.62 L114.26 1388.62 L114.26 1384.69 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M166.876 1371.03 L155.07 1389.48 L166.876 1389.48 L166.876 1371.03 M165.649 1366.96 L171.528 1366.96 L171.528 1389.48 L176.459 1389.48 L176.459 1393.37 L171.528 1393.37 L171.528 1401.52 L166.876 1401.52 L166.876 1393.37 L151.274 1393.37 L151.274 1388.86 L165.649 1366.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M115.394 952.005 L145.07 952.005 L145.07 955.941 L115.394 955.941 L115.394 952.005 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M169.329 950.2 Q172.686 950.917 174.561 953.186 Q176.459 955.455 176.459 958.788 Q176.459 963.904 172.94 966.704 Q169.422 969.505 162.94 969.505 Q160.765 969.505 158.45 969.066 Q156.158 968.649 153.704 967.792 L153.704 963.279 Q155.649 964.413 157.964 964.991 Q160.278 965.57 162.802 965.57 Q167.2 965.57 169.491 963.834 Q171.806 962.098 171.806 958.788 Q171.806 955.732 169.653 954.019 Q167.524 952.283 163.704 952.283 L159.677 952.283 L159.677 948.441 L163.89 948.441 Q167.339 948.441 169.167 947.075 Q170.996 945.686 170.996 943.093 Q170.996 940.431 169.098 939.019 Q167.223 937.584 163.704 937.584 Q161.783 937.584 159.584 938.001 Q157.385 938.418 154.746 939.297 L154.746 935.131 Q157.408 934.39 159.723 934.019 Q162.061 933.649 164.121 933.649 Q169.445 933.649 172.547 936.08 Q175.649 938.487 175.649 942.607 Q175.649 945.478 174.005 947.468 Q172.362 949.436 169.329 950.2 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M116.343 519.321 L146.019 519.321 L146.019 523.256 L116.343 523.256 L116.343 519.321 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M160.14 532.215 L176.459 532.215 L176.459 536.15 L154.515 536.15 L154.515 532.215 Q157.177 529.46 161.76 524.83 Q166.366 520.178 167.547 518.835 Q169.792 516.312 170.672 514.576 Q171.575 512.817 171.575 511.127 Q171.575 508.372 169.63 506.636 Q167.709 504.9 164.607 504.9 Q162.408 504.9 159.954 505.664 Q157.524 506.428 154.746 507.979 L154.746 503.257 Q157.57 502.122 160.024 501.544 Q162.477 500.965 164.515 500.965 Q169.885 500.965 173.079 503.65 Q176.274 506.335 176.274 510.826 Q176.274 512.956 175.464 514.877 Q174.677 516.775 172.57 519.368 Q171.991 520.039 168.889 523.256 Q165.788 526.451 160.14 532.215 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M115.973 86.6371 L145.649 86.6371 L145.649 90.5722 L115.973 90.5722 L115.973 86.6371 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M156.552 99.5305 L164.19 99.5305 L164.19 73.1649 L155.88 74.8316 L155.88 70.5723 L164.144 68.9057 L168.82 68.9057 L168.82 99.5305 L176.459 99.5305 L176.459 103.466 L156.552 103.466 L156.552 99.5305 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M16.4842 940.968 L16.4842 913.659 L21.895 913.659 L21.895 934.538 L35.8996 934.538 L35.8996 915.696 L41.3104 915.696 L41.3104 934.538 L64.0042 934.538 L64.0042 940.968 L16.4842 940.968 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M28.3562 908.471 L28.3562 902.614 L64.0042 902.614 L64.0042 908.471 L28.3562 908.471 M14.479 908.471 L14.479 902.614 L21.895 902.614 L21.895 908.471 L14.479 908.471 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.8307 869.703 Q33.2578 870.69 33.0032 871.868 Q32.7167 873.014 32.7167 874.414 Q32.7167 879.379 35.9632 882.053 Q39.1779 884.695 45.2253 884.695 L64.0042 884.695 L64.0042 890.583 L28.3562 890.583 L28.3562 884.695 L33.8944 884.695 Q30.6479 882.849 29.0883 879.889 Q27.4968 876.929 27.4968 872.695 Q27.4968 872.091 27.5923 871.359 Q27.656 870.626 27.8151 869.735 L33.8307 869.703 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M29.4065 840.835 L34.9447 840.835 Q33.6716 843.318 33.035 845.991 Q32.3984 848.665 32.3984 851.529 Q32.3984 855.89 33.7352 858.086 Q35.072 860.25 37.7456 860.25 Q39.7826 860.25 40.9603 858.691 Q42.1061 857.131 43.1565 852.421 L43.6021 850.415 Q44.9389 844.177 47.3897 841.567 Q49.8086 838.925 54.1691 838.925 Q59.1344 838.925 62.0308 842.872 Q64.9272 846.787 64.9272 853.662 Q64.9272 856.526 64.3543 859.646 Q63.8132 862.733 62.6992 866.17 L56.6518 866.17 Q58.3387 862.924 59.198 859.773 Q60.0256 856.622 60.0256 853.535 Q60.0256 849.397 58.6251 847.169 Q57.1929 844.941 54.6147 844.941 Q52.2276 844.941 50.9545 846.564 Q49.6813 848.156 48.5037 853.598 L48.0262 855.635 Q46.8804 861.078 44.5251 863.497 Q42.138 865.916 38.0002 865.916 Q32.9713 865.916 30.2341 862.351 Q27.4968 858.786 27.4968 852.23 Q27.4968 848.983 27.9743 846.118 Q28.4517 843.254 29.4065 840.835 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M18.2347 823.807 L28.3562 823.807 L28.3562 811.744 L32.9077 811.744 L32.9077 823.807 L52.2594 823.807 Q56.6199 823.807 57.8613 822.629 Q59.1026 821.42 59.1026 817.759 L59.1026 811.744 L64.0042 811.744 L64.0042 817.759 Q64.0042 824.539 61.4897 827.117 Q58.9434 829.695 52.2594 829.695 L32.9077 829.695 L32.9077 833.992 L28.3562 833.992 L28.3562 829.695 L18.2347 829.695 L18.2347 823.807 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M18.0438 754.58 L24.314 754.58 Q22.5634 758.24 21.704 761.486 Q20.8447 764.733 20.8447 767.757 Q20.8447 773.008 22.8817 775.873 Q24.9187 778.706 28.6745 778.706 Q31.8255 778.706 33.4488 776.828 Q35.0402 774.918 36.0269 769.635 L36.8226 765.751 Q38.1912 758.558 41.6605 755.153 Q45.098 751.715 50.8908 751.715 Q57.7976 751.715 61.3624 756.362 Q64.9272 760.977 64.9272 769.921 Q64.9272 773.295 64.1633 777.114 Q63.3994 780.902 61.9035 784.976 L55.2831 784.976 Q57.4793 781.061 58.5933 777.305 Q59.7073 773.549 59.7073 769.921 Q59.7073 764.415 57.543 761.423 Q55.3786 758.431 51.3682 758.431 Q47.8671 758.431 45.8937 760.595 Q43.9204 762.728 42.9337 767.629 L42.1698 771.544 Q40.7375 778.737 37.682 781.952 Q34.6264 785.167 29.1837 785.167 Q22.8817 785.167 19.2532 780.743 Q15.6248 776.287 15.6248 768.489 Q15.6248 765.147 16.2295 761.677 Q16.8343 758.208 18.0438 754.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M32.4621 728.13 Q32.4621 732.841 36.1542 735.578 Q39.8145 738.315 46.212 738.315 Q52.6095 738.315 56.3017 735.61 Q59.9619 732.873 59.9619 728.13 Q59.9619 723.451 56.2698 720.714 Q52.5777 717.977 46.212 717.977 Q39.8781 717.977 36.186 720.714 Q32.4621 723.451 32.4621 728.13 M27.4968 728.13 Q27.4968 720.491 32.4621 716.131 Q37.4273 711.77 46.212 711.77 Q54.9649 711.77 59.9619 716.131 Q64.9272 720.491 64.9272 728.13 Q64.9272 735.801 59.9619 740.161 Q54.9649 744.49 46.212 744.49 Q37.4273 744.49 32.4621 740.161 Q27.4968 735.801 27.4968 728.13 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M14.479 702.063 L14.479 696.206 L64.0042 696.206 L64.0042 702.063 L14.479 702.063 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M49.9359 684.557 L28.3562 684.557 L28.3562 678.7 L49.7131 678.7 Q54.7739 678.7 57.3202 676.727 Q59.8346 674.754 59.8346 670.807 Q59.8346 666.064 56.8109 663.327 Q53.7872 660.558 48.5673 660.558 L28.3562 660.558 L28.3562 654.702 L64.0042 654.702 L64.0042 660.558 L58.5296 660.558 Q61.7762 662.691 63.3676 665.523 Q64.9272 668.324 64.9272 672.048 Q64.9272 678.191 61.1078 681.374 Q57.2883 684.557 49.9359 684.557 M27.4968 669.82 L27.4968 669.82 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M18.2347 636.846 L28.3562 636.846 L28.3562 624.783 L32.9077 624.783 L32.9077 636.846 L52.2594 636.846 Q56.6199 636.846 57.8613 635.668 Q59.1026 634.459 59.1026 630.798 L59.1026 624.783 L64.0042 624.783 L64.0042 630.798 Q64.0042 637.578 61.4897 640.156 Q58.9434 642.734 52.2594 642.734 L32.9077 642.734 L32.9077 647.031 L28.3562 647.031 L28.3562 642.734 L18.2347 642.734 L18.2347 636.846 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M28.3562 617.08 L28.3562 611.224 L64.0042 611.224 L64.0042 617.08 L28.3562 617.08 M14.479 617.08 L14.479 611.224 L21.895 611.224 L21.895 617.08 L14.479 617.08 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M32.4621 585.156 Q32.4621 589.867 36.1542 592.604 Q39.8145 595.341 46.212 595.341 Q52.6095 595.341 56.3017 592.636 Q59.9619 589.899 59.9619 585.156 Q59.9619 580.478 56.2698 577.74 Q52.5777 575.003 46.212 575.003 Q39.8781 575.003 36.186 577.74 Q32.4621 580.478 32.4621 585.156 M27.4968 585.156 Q27.4968 577.518 32.4621 573.157 Q37.4273 568.796 46.212 568.796 Q54.9649 568.796 59.9619 573.157 Q64.9272 577.518 64.9272 585.156 Q64.9272 592.827 59.9619 597.188 Q54.9649 601.516 46.212 601.516 Q37.4273 601.516 32.4621 597.188 Q27.4968 592.827 27.4968 585.156 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M42.4881 529.456 L64.0042 529.456 L64.0042 535.313 L42.679 535.313 Q37.6183 535.313 35.1038 537.286 Q32.5894 539.26 32.5894 543.206 Q32.5894 547.949 35.6131 550.686 Q38.6368 553.423 43.8567 553.423 L64.0042 553.423 L64.0042 559.312 L28.3562 559.312 L28.3562 553.423 L33.8944 553.423 Q30.6797 551.323 29.0883 548.49 Q27.4968 545.625 27.4968 541.901 Q27.4968 535.758 31.3163 532.607 Q35.1038 529.456 42.4881 529.456 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip942)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:8; stroke-opacity:1; fill:none\" points=\"273.033,951.554 379.304,951.554 485.575,1384.24 591.846,951.554 698.117,951.554 804.388,951.554 910.659,1384.24 1016.93,1384.24 1123.2,1384.24 1229.47,518.87 1335.74,951.554 1442.01,951.554 1548.28,1384.24 1654.56,951.554 1760.83,951.554 1867.1,1384.24 1973.37,951.554 2079.64,1384.24 2185.91,951.554 2292.18,518.87 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"273.033,951.554 379.304,951.554 485.575,1384.24 591.846,951.554 698.117,951.554 804.388,951.554 910.659,1384.24 1016.93,1384.24 1123.2,1384.24 1229.47,518.87 1335.74,951.554 1442.01,951.554 1548.28,1384.24 1654.56,951.554 1760.83,951.554 1867.1,1384.24 1973.37,951.554 2079.64,1384.24 2185.91,951.554 2292.18,518.87 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"273.033,86.1857 379.304,518.87 485.575,1384.24 591.846,518.87 698.117,518.87 804.388,518.87 910.659,518.87 1016.93,951.554 1123.2,86.1857 1229.47,518.87 1335.74,951.554 1442.01,951.554 1548.28,518.87 1654.56,518.87 1760.83,518.87 1867.1,518.87 1973.37,518.87 2079.64,518.87 2185.91,518.87 2292.18,86.1857 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M1201.95 300.469 L2281.41 300.469 L2281.41 93.1086 L1201.95 93.1086  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1201.95,300.469 2281.41,300.469 2281.41,93.1086 1201.95,93.1086 1201.95,300.469 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:8; stroke-opacity:1; fill:none\" points=\"1225.73,144.949 1368.42,144.949 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M1414.44 128.803 L1414.44 133.363 Q1411.78 132.09 1409.42 131.465 Q1407.06 130.84 1404.86 130.84 Q1401.04 130.84 1398.96 132.321 Q1396.9 133.803 1396.9 136.534 Q1396.9 138.826 1398.26 140.007 Q1399.65 141.164 1403.49 141.882 L1406.32 142.46 Q1411.55 143.456 1414.03 145.979 Q1416.52 148.479 1416.52 152.692 Q1416.52 157.715 1413.15 160.307 Q1409.79 162.9 1403.28 162.9 Q1400.83 162.9 1398.05 162.344 Q1395.3 161.789 1392.34 160.701 L1392.34 155.886 Q1395.18 157.483 1397.91 158.293 Q1400.65 159.104 1403.28 159.104 Q1407.29 159.104 1409.46 157.53 Q1411.64 155.956 1411.64 153.039 Q1411.64 150.493 1410.07 149.057 Q1408.52 147.622 1404.95 146.905 L1402.1 146.349 Q1396.87 145.307 1394.53 143.085 Q1392.2 140.863 1392.2 136.905 Q1392.2 132.321 1395.41 129.682 Q1398.65 127.044 1404.33 127.044 Q1406.76 127.044 1409.28 127.483 Q1411.8 127.923 1414.44 128.803 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1423.63 136.303 L1427.89 136.303 L1427.89 162.229 L1423.63 162.229 L1423.63 136.303 M1423.63 126.21 L1427.89 126.21 L1427.89 131.604 L1423.63 131.604 L1423.63 126.21 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1456.99 141.28 Q1458.58 138.409 1460.81 137.044 Q1463.03 135.678 1466.04 135.678 Q1470.09 135.678 1472.29 138.525 Q1474.49 141.349 1474.49 146.581 L1474.49 162.229 L1470.21 162.229 L1470.21 146.719 Q1470.21 142.993 1468.89 141.187 Q1467.57 139.382 1464.86 139.382 Q1461.55 139.382 1459.63 141.581 Q1457.71 143.78 1457.71 147.576 L1457.71 162.229 L1453.42 162.229 L1453.42 146.719 Q1453.42 142.969 1452.1 141.187 Q1450.78 139.382 1448.03 139.382 Q1444.77 139.382 1442.84 141.604 Q1440.92 143.803 1440.92 147.576 L1440.92 162.229 L1436.64 162.229 L1436.64 136.303 L1440.92 136.303 L1440.92 140.331 Q1442.38 137.946 1444.42 136.812 Q1446.46 135.678 1449.26 135.678 Q1452.08 135.678 1454.05 137.113 Q1456.04 138.548 1456.99 141.28 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1487.1 158.34 L1487.1 172.09 L1482.82 172.09 L1482.82 136.303 L1487.1 136.303 L1487.1 140.238 Q1488.45 137.923 1490.48 136.812 Q1492.54 135.678 1495.39 135.678 Q1500.11 135.678 1503.05 139.428 Q1506.02 143.178 1506.02 149.289 Q1506.02 155.4 1503.05 159.15 Q1500.11 162.9 1495.39 162.9 Q1492.54 162.9 1490.48 161.789 Q1488.45 160.655 1487.1 158.34 M1501.59 149.289 Q1501.59 144.59 1499.65 141.928 Q1497.73 139.243 1494.35 139.243 Q1490.97 139.243 1489.02 141.928 Q1487.1 144.59 1487.1 149.289 Q1487.1 153.988 1489.02 156.673 Q1490.97 159.335 1494.35 159.335 Q1497.73 159.335 1499.65 156.673 Q1501.59 153.988 1501.59 149.289 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1513.08 126.21 L1517.33 126.21 L1517.33 162.229 L1513.08 162.229 L1513.08 126.21 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1548.42 148.201 L1548.42 150.284 L1528.84 150.284 Q1529.12 154.682 1531.48 156.997 Q1533.86 159.289 1538.1 159.289 Q1540.55 159.289 1542.84 158.687 Q1545.16 158.085 1547.43 156.881 L1547.43 160.909 Q1545.14 161.881 1542.73 162.391 Q1540.32 162.9 1537.84 162.9 Q1531.64 162.9 1528.01 159.289 Q1524.39 155.678 1524.39 149.52 Q1524.39 143.155 1527.82 139.428 Q1531.27 135.678 1537.1 135.678 Q1542.33 135.678 1545.37 139.057 Q1548.42 142.414 1548.42 148.201 M1544.16 146.951 Q1544.12 143.456 1542.2 141.372 Q1540.3 139.289 1537.15 139.289 Q1533.58 139.289 1531.43 141.303 Q1529.3 143.317 1528.98 146.974 L1544.16 146.951 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1555.6 127.669 L1560.27 127.669 L1560.27 158.293 L1577.1 158.293 L1577.1 162.229 L1555.6 162.229 L1555.6 127.669 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1603.17 148.201 L1603.17 150.284 L1583.58 150.284 Q1583.86 154.682 1586.22 156.997 Q1588.61 159.289 1592.84 159.289 Q1595.3 159.289 1597.59 158.687 Q1599.9 158.085 1602.17 156.881 L1602.17 160.909 Q1599.88 161.881 1597.47 162.391 Q1595.07 162.9 1592.59 162.9 Q1586.39 162.9 1582.75 159.289 Q1579.14 155.678 1579.14 149.52 Q1579.14 143.155 1582.57 139.428 Q1586.01 135.678 1591.85 135.678 Q1597.08 135.678 1600.11 139.057 Q1603.17 142.414 1603.17 148.201 M1598.91 146.951 Q1598.86 143.456 1596.94 141.372 Q1595.04 139.289 1591.89 139.289 Q1588.33 139.289 1586.18 141.303 Q1584.05 143.317 1583.72 146.974 L1598.91 146.951 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1621.94 149.196 Q1616.78 149.196 1614.79 150.377 Q1612.8 151.557 1612.8 154.405 Q1612.8 156.673 1614.28 158.016 Q1615.78 159.335 1618.35 159.335 Q1621.89 159.335 1624.02 156.835 Q1626.18 154.312 1626.18 150.145 L1626.18 149.196 L1621.94 149.196 M1630.44 147.437 L1630.44 162.229 L1626.18 162.229 L1626.18 158.293 Q1624.72 160.655 1622.54 161.789 Q1620.37 162.9 1617.22 162.9 Q1613.24 162.9 1610.88 160.678 Q1608.54 158.432 1608.54 154.682 Q1608.54 150.307 1611.45 148.085 Q1614.39 145.863 1620.2 145.863 L1626.18 145.863 L1626.18 145.446 Q1626.18 142.507 1624.23 140.909 Q1622.31 139.289 1618.82 139.289 Q1616.59 139.289 1614.49 139.821 Q1612.38 140.354 1610.44 141.419 L1610.44 137.483 Q1612.77 136.581 1614.97 136.141 Q1617.17 135.678 1619.26 135.678 Q1624.88 135.678 1627.66 138.594 Q1630.44 141.511 1630.44 147.437 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1654.23 140.284 Q1653.51 139.868 1652.66 139.682 Q1651.82 139.474 1650.81 139.474 Q1647.2 139.474 1645.25 141.835 Q1643.33 144.173 1643.33 148.571 L1643.33 162.229 L1639.05 162.229 L1639.05 136.303 L1643.33 136.303 L1643.33 140.331 Q1644.67 137.969 1646.82 136.835 Q1648.98 135.678 1652.06 135.678 Q1652.5 135.678 1653.03 135.747 Q1653.56 135.794 1654.21 135.909 L1654.23 140.284 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1679.42 146.581 L1679.42 162.229 L1675.16 162.229 L1675.16 146.719 Q1675.16 143.039 1673.72 141.21 Q1672.29 139.382 1669.42 139.382 Q1665.97 139.382 1663.98 141.581 Q1661.99 143.78 1661.99 147.576 L1661.99 162.229 L1657.7 162.229 L1657.7 136.303 L1661.99 136.303 L1661.99 140.331 Q1663.51 137.993 1665.57 136.835 Q1667.66 135.678 1670.37 135.678 Q1674.83 135.678 1677.13 138.456 Q1679.42 141.21 1679.42 146.581 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1710.09 148.201 L1710.09 150.284 L1690.51 150.284 Q1690.78 154.682 1693.14 156.997 Q1695.53 159.289 1699.76 159.289 Q1702.22 159.289 1704.51 158.687 Q1706.82 158.085 1709.09 156.881 L1709.09 160.909 Q1706.8 161.881 1704.39 162.391 Q1701.99 162.9 1699.51 162.9 Q1693.31 162.9 1689.67 159.289 Q1686.06 155.678 1686.06 149.52 Q1686.06 143.155 1689.49 139.428 Q1692.94 135.678 1698.77 135.678 Q1704 135.678 1707.03 139.057 Q1710.09 142.414 1710.09 148.201 M1705.83 146.951 Q1705.78 143.456 1703.86 141.372 Q1701.96 139.289 1698.82 139.289 Q1695.25 139.289 1693.1 141.303 Q1690.97 143.317 1690.64 146.974 L1705.83 146.951 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1734.14 140.238 L1734.14 126.21 L1738.4 126.21 L1738.4 162.229 L1734.14 162.229 L1734.14 158.34 Q1732.8 160.655 1730.74 161.789 Q1728.7 162.9 1725.83 162.9 Q1721.13 162.9 1718.17 159.15 Q1715.23 155.4 1715.23 149.289 Q1715.23 143.178 1718.17 139.428 Q1721.13 135.678 1725.83 135.678 Q1728.7 135.678 1730.74 136.812 Q1732.8 137.923 1734.14 140.238 M1719.63 149.289 Q1719.63 153.988 1721.55 156.673 Q1723.49 159.335 1726.87 159.335 Q1730.25 159.335 1732.19 156.673 Q1734.14 153.988 1734.14 149.289 Q1734.14 144.59 1732.19 141.928 Q1730.25 139.243 1726.87 139.243 Q1723.49 139.243 1721.55 141.928 Q1719.63 144.59 1719.63 149.289 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1747.36 127.669 L1752.03 127.669 L1752.03 141.835 L1769.02 141.835 L1769.02 127.669 L1773.7 127.669 L1773.7 162.229 L1769.02 162.229 L1769.02 145.77 L1752.03 145.77 L1752.03 162.229 L1747.36 162.229 L1747.36 127.669 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1805 148.201 L1805 150.284 L1785.41 150.284 Q1785.69 154.682 1788.05 156.997 Q1790.44 159.289 1794.67 159.289 Q1797.12 159.289 1799.42 158.687 Q1801.73 158.085 1804 156.881 L1804 160.909 Q1801.71 161.881 1799.3 162.391 Q1796.89 162.9 1794.42 162.9 Q1788.21 162.9 1784.58 159.289 Q1780.97 155.678 1780.97 149.52 Q1780.97 143.155 1784.39 139.428 Q1787.84 135.678 1793.68 135.678 Q1798.91 135.678 1801.94 139.057 Q1805 142.414 1805 148.201 M1800.74 146.951 Q1800.69 143.456 1798.77 141.372 Q1796.87 139.289 1793.72 139.289 Q1790.16 139.289 1788 141.303 Q1785.87 143.317 1785.55 146.974 L1800.74 146.951 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1811.55 151.997 L1811.55 136.303 L1815.81 136.303 L1815.81 151.835 Q1815.81 155.516 1817.24 157.368 Q1818.68 159.196 1821.55 159.196 Q1825 159.196 1826.99 156.997 Q1829 154.798 1829 151.002 L1829 136.303 L1833.26 136.303 L1833.26 162.229 L1829 162.229 L1829 158.247 Q1827.45 160.608 1825.39 161.766 Q1823.35 162.9 1820.64 162.9 Q1816.18 162.9 1813.86 160.122 Q1811.55 157.344 1811.55 151.997 M1822.26 135.678 L1822.26 135.678 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1857.06 140.284 Q1856.34 139.868 1855.48 139.682 Q1854.65 139.474 1853.63 139.474 Q1850.02 139.474 1848.07 141.835 Q1846.15 144.173 1846.15 148.571 L1846.15 162.229 L1841.87 162.229 L1841.87 136.303 L1846.15 136.303 L1846.15 140.331 Q1847.49 137.969 1849.65 136.835 Q1851.8 135.678 1854.88 135.678 Q1855.32 135.678 1855.85 135.747 Q1856.38 135.794 1857.03 135.909 L1857.06 140.284 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1861.52 136.303 L1865.78 136.303 L1865.78 162.229 L1861.52 162.229 L1861.52 136.303 M1861.52 126.21 L1865.78 126.21 L1865.78 131.604 L1861.52 131.604 L1861.52 126.21 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1891.22 137.067 L1891.22 141.094 Q1889.42 140.169 1887.47 139.706 Q1885.53 139.243 1883.44 139.243 Q1880.27 139.243 1878.68 140.215 Q1877.1 141.187 1877.1 143.131 Q1877.1 144.613 1878.24 145.469 Q1879.37 146.303 1882.8 147.067 L1884.25 147.391 Q1888.79 148.363 1890.69 150.145 Q1892.61 151.905 1892.61 155.076 Q1892.61 158.687 1889.74 160.793 Q1886.89 162.9 1881.89 162.9 Q1879.81 162.9 1877.54 162.483 Q1875.3 162.09 1872.8 161.28 L1872.8 156.881 Q1875.16 158.108 1877.45 158.733 Q1879.74 159.335 1881.99 159.335 Q1884.99 159.335 1886.62 158.317 Q1888.24 157.275 1888.24 155.4 Q1888.24 153.664 1887.05 152.738 Q1885.9 151.812 1881.94 150.956 L1880.46 150.608 Q1876.5 149.775 1874.74 148.062 Q1872.98 146.326 1872.98 143.317 Q1872.98 139.659 1875.57 137.669 Q1878.17 135.678 1882.93 135.678 Q1885.3 135.678 1887.38 136.025 Q1889.46 136.372 1891.22 137.067 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1903.61 128.942 L1903.61 136.303 L1912.38 136.303 L1912.38 139.613 L1903.61 139.613 L1903.61 153.687 Q1903.61 156.858 1904.46 157.761 Q1905.34 158.664 1908 158.664 L1912.38 158.664 L1912.38 162.229 L1908 162.229 Q1903.07 162.229 1901.2 160.4 Q1899.32 158.548 1899.32 153.687 L1899.32 139.613 L1896.2 139.613 L1896.2 136.303 L1899.32 136.303 L1899.32 128.942 L1903.61 128.942 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1917.98 136.303 L1922.24 136.303 L1922.24 162.229 L1917.98 162.229 L1917.98 136.303 M1917.98 126.21 L1922.24 126.21 L1922.24 131.604 L1917.98 131.604 L1917.98 126.21 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1949.81 137.298 L1949.81 141.28 Q1948 140.284 1946.17 139.798 Q1944.37 139.289 1942.52 139.289 Q1938.37 139.289 1936.08 141.928 Q1933.79 144.544 1933.79 149.289 Q1933.79 154.034 1936.08 156.673 Q1938.37 159.289 1942.52 159.289 Q1944.37 159.289 1946.17 158.803 Q1948 158.293 1949.81 157.298 L1949.81 161.233 Q1948.03 162.067 1946.11 162.483 Q1944.21 162.9 1942.05 162.9 Q1936.2 162.9 1932.75 159.219 Q1929.3 155.539 1929.3 149.289 Q1929.3 142.946 1932.77 139.312 Q1936.27 135.678 1942.33 135.678 Q1944.3 135.678 1946.17 136.095 Q1948.05 136.488 1949.81 137.298 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip940)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1225.73,196.789 1368.42,196.789 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M1396.87 197.564 L1396.87 210.226 L1404.37 210.226 Q1408.15 210.226 1409.95 208.675 Q1411.78 207.101 1411.78 203.883 Q1411.78 200.643 1409.95 199.115 Q1408.15 197.564 1404.37 197.564 L1396.87 197.564 M1396.87 183.351 L1396.87 193.768 L1403.79 193.768 Q1407.22 193.768 1408.89 192.495 Q1410.58 191.198 1410.58 188.559 Q1410.58 185.944 1408.89 184.647 Q1407.22 183.351 1403.79 183.351 L1396.87 183.351 M1392.2 179.509 L1404.14 179.509 Q1409.49 179.509 1412.38 181.731 Q1415.27 183.953 1415.27 188.05 Q1415.27 191.222 1413.79 193.097 Q1412.31 194.971 1409.44 195.434 Q1412.89 196.175 1414.79 198.536 Q1416.71 200.874 1416.71 204.393 Q1416.71 209.022 1413.56 211.545 Q1410.41 214.069 1404.6 214.069 L1392.2 214.069 L1392.2 179.509 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1436.32 201.036 Q1431.15 201.036 1429.16 202.217 Q1427.17 203.397 1427.17 206.245 Q1427.17 208.513 1428.65 209.856 Q1430.16 211.175 1432.73 211.175 Q1436.27 211.175 1438.4 208.675 Q1440.55 206.152 1440.55 201.985 L1440.55 201.036 L1436.32 201.036 M1444.81 199.277 L1444.81 214.069 L1440.55 214.069 L1440.55 210.133 Q1439.09 212.495 1436.92 213.629 Q1434.74 214.74 1431.59 214.74 Q1427.61 214.74 1425.25 212.518 Q1422.91 210.272 1422.91 206.522 Q1422.91 202.147 1425.83 199.925 Q1428.77 197.703 1434.58 197.703 L1440.55 197.703 L1440.55 197.286 Q1440.55 194.347 1438.61 192.749 Q1436.69 191.129 1433.19 191.129 Q1430.97 191.129 1428.86 191.661 Q1426.76 192.194 1424.81 193.259 L1424.81 189.323 Q1427.15 188.421 1429.35 187.981 Q1431.55 187.518 1433.63 187.518 Q1439.26 187.518 1442.03 190.434 Q1444.81 193.351 1444.81 199.277 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1470.11 188.907 L1470.11 192.934 Q1468.31 192.009 1466.36 191.546 Q1464.42 191.083 1462.33 191.083 Q1459.16 191.083 1457.57 192.055 Q1455.99 193.027 1455.99 194.971 Q1455.99 196.453 1457.13 197.309 Q1458.26 198.143 1461.69 198.907 L1463.15 199.231 Q1467.68 200.203 1469.58 201.985 Q1471.5 203.745 1471.5 206.916 Q1471.5 210.527 1468.63 212.633 Q1465.78 214.74 1460.78 214.74 Q1458.7 214.74 1456.43 214.323 Q1454.19 213.93 1451.69 213.12 L1451.69 208.721 Q1454.05 209.948 1456.34 210.573 Q1458.63 211.175 1460.88 211.175 Q1463.89 211.175 1465.51 210.157 Q1467.13 209.115 1467.13 207.24 Q1467.13 205.504 1465.95 204.578 Q1464.79 203.652 1460.83 202.796 L1459.35 202.448 Q1455.39 201.615 1453.63 199.902 Q1451.87 198.166 1451.87 195.157 Q1451.87 191.499 1454.46 189.509 Q1457.06 187.518 1461.83 187.518 Q1464.19 187.518 1466.27 187.865 Q1468.35 188.212 1470.11 188.907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1478.28 188.143 L1482.54 188.143 L1482.54 214.069 L1478.28 214.069 L1478.28 188.143 M1478.28 178.05 L1482.54 178.05 L1482.54 183.444 L1478.28 183.444 L1478.28 178.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1510.11 189.138 L1510.11 193.12 Q1508.31 192.124 1506.48 191.638 Q1504.67 191.129 1502.82 191.129 Q1498.68 191.129 1496.39 193.768 Q1494.09 196.384 1494.09 201.129 Q1494.09 205.874 1496.39 208.513 Q1498.68 211.129 1502.82 211.129 Q1504.67 211.129 1506.48 210.643 Q1508.31 210.133 1510.11 209.138 L1510.11 213.073 Q1508.33 213.907 1506.41 214.323 Q1504.51 214.74 1502.36 214.74 Q1496.5 214.74 1493.05 211.059 Q1489.6 207.379 1489.6 201.129 Q1489.6 194.786 1493.08 191.152 Q1496.57 187.518 1502.64 187.518 Q1504.6 187.518 1506.48 187.935 Q1508.35 188.328 1510.11 189.138 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1517.71 179.509 L1522.38 179.509 L1522.38 193.675 L1539.37 193.675 L1539.37 179.509 L1544.05 179.509 L1544.05 214.069 L1539.37 214.069 L1539.37 197.61 L1522.38 197.61 L1522.38 214.069 L1517.71 214.069 L1517.71 179.509 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1575.34 200.041 L1575.34 202.124 L1555.76 202.124 Q1556.04 206.522 1558.4 208.837 Q1560.78 211.129 1565.02 211.129 Q1567.47 211.129 1569.76 210.527 Q1572.08 209.925 1574.35 208.721 L1574.35 212.749 Q1572.06 213.721 1569.65 214.231 Q1567.24 214.74 1564.76 214.74 Q1558.56 214.74 1554.93 211.129 Q1551.32 207.518 1551.32 201.36 Q1551.32 194.995 1554.74 191.268 Q1558.19 187.518 1564.02 187.518 Q1569.26 187.518 1572.29 190.897 Q1575.34 194.254 1575.34 200.041 M1571.08 198.791 Q1571.04 195.296 1569.12 193.212 Q1567.22 191.129 1564.07 191.129 Q1560.51 191.129 1558.35 193.143 Q1556.22 195.157 1555.9 198.814 L1571.08 198.791 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1581.89 203.837 L1581.89 188.143 L1586.15 188.143 L1586.15 203.675 Q1586.15 207.356 1587.59 209.208 Q1589.02 211.036 1591.89 211.036 Q1595.34 211.036 1597.33 208.837 Q1599.35 206.638 1599.35 202.842 L1599.35 188.143 L1603.61 188.143 L1603.61 214.069 L1599.35 214.069 L1599.35 210.087 Q1597.8 212.448 1595.74 213.606 Q1593.7 214.74 1590.99 214.74 Q1586.52 214.74 1584.21 211.962 Q1581.89 209.184 1581.89 203.837 M1592.61 187.518 L1592.61 187.518 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1627.4 192.124 Q1626.69 191.708 1625.83 191.522 Q1625 191.314 1623.98 191.314 Q1620.37 191.314 1618.42 193.675 Q1616.5 196.013 1616.5 200.411 L1616.5 214.069 L1612.22 214.069 L1612.22 188.143 L1616.5 188.143 L1616.5 192.171 Q1617.84 189.809 1620 188.675 Q1622.15 187.518 1625.23 187.518 Q1625.67 187.518 1626.2 187.587 Q1626.73 187.634 1627.38 187.749 L1627.4 192.124 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1631.87 188.143 L1636.13 188.143 L1636.13 214.069 L1631.87 214.069 L1631.87 188.143 M1631.87 178.05 L1636.13 178.05 L1636.13 183.444 L1631.87 183.444 L1631.87 178.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1661.57 188.907 L1661.57 192.934 Q1659.76 192.009 1657.82 191.546 Q1655.88 191.083 1653.79 191.083 Q1650.62 191.083 1649.02 192.055 Q1647.45 193.027 1647.45 194.971 Q1647.45 196.453 1648.58 197.309 Q1649.72 198.143 1653.14 198.907 L1654.6 199.231 Q1659.14 200.203 1661.04 201.985 Q1662.96 203.745 1662.96 206.916 Q1662.96 210.527 1660.09 212.633 Q1657.24 214.74 1652.24 214.74 Q1650.16 214.74 1647.89 214.323 Q1645.64 213.93 1643.14 213.12 L1643.14 208.721 Q1645.51 209.948 1647.8 210.573 Q1650.09 211.175 1652.33 211.175 Q1655.34 211.175 1656.96 210.157 Q1658.58 209.115 1658.58 207.24 Q1658.58 205.504 1657.4 204.578 Q1656.25 203.652 1652.29 202.796 L1650.81 202.448 Q1646.85 201.615 1645.09 199.902 Q1643.33 198.166 1643.33 195.157 Q1643.33 191.499 1645.92 189.509 Q1648.51 187.518 1653.28 187.518 Q1655.64 187.518 1657.73 187.865 Q1659.81 188.212 1661.57 188.907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1673.95 180.782 L1673.95 188.143 L1682.73 188.143 L1682.73 191.453 L1673.95 191.453 L1673.95 205.527 Q1673.95 208.698 1674.81 209.601 Q1675.69 210.504 1678.35 210.504 L1682.73 210.504 L1682.73 214.069 L1678.35 214.069 Q1673.42 214.069 1671.55 212.24 Q1669.67 210.388 1669.67 205.527 L1669.67 191.453 L1666.55 191.453 L1666.55 188.143 L1669.67 188.143 L1669.67 180.782 L1673.95 180.782 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1688.33 188.143 L1692.59 188.143 L1692.59 214.069 L1688.33 214.069 L1688.33 188.143 M1688.33 178.05 L1692.59 178.05 L1692.59 183.444 L1688.33 183.444 L1688.33 178.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1720.16 189.138 L1720.16 193.12 Q1718.35 192.124 1716.52 191.638 Q1714.72 191.129 1712.87 191.129 Q1708.72 191.129 1706.43 193.768 Q1704.14 196.384 1704.14 201.129 Q1704.14 205.874 1706.43 208.513 Q1708.72 211.129 1712.87 211.129 Q1714.72 211.129 1716.52 210.643 Q1718.35 210.133 1720.16 209.138 L1720.16 213.073 Q1718.38 213.907 1716.45 214.323 Q1714.56 214.74 1712.4 214.74 Q1706.55 214.74 1703.1 211.059 Q1699.65 207.379 1699.65 201.129 Q1699.65 194.786 1703.12 191.152 Q1706.62 187.518 1712.68 187.518 Q1714.65 187.518 1716.52 187.935 Q1718.4 188.328 1720.16 189.138 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1737.8 178.097 Q1734.69 183.421 1733.19 188.629 Q1731.69 193.837 1731.69 199.184 Q1731.69 204.532 1733.19 209.786 Q1734.72 215.018 1737.8 220.319 L1734.09 220.319 Q1730.62 214.879 1728.88 209.624 Q1727.17 204.37 1727.17 199.184 Q1727.17 194.022 1728.88 188.791 Q1730.6 183.56 1734.09 178.097 L1737.8 178.097 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1762.59 188.907 L1762.59 192.934 Q1760.78 192.009 1758.84 191.546 Q1756.89 191.083 1754.81 191.083 Q1751.64 191.083 1750.04 192.055 Q1748.47 193.027 1748.47 194.971 Q1748.47 196.453 1749.6 197.309 Q1750.74 198.143 1754.16 198.907 L1755.62 199.231 Q1760.16 200.203 1762.06 201.985 Q1763.98 203.745 1763.98 206.916 Q1763.98 210.527 1761.11 212.633 Q1758.26 214.74 1753.26 214.74 Q1751.18 214.74 1748.91 214.323 Q1746.66 213.93 1744.16 213.12 L1744.16 208.721 Q1746.52 209.948 1748.81 210.573 Q1751.11 211.175 1753.35 211.175 Q1756.36 211.175 1757.98 210.157 Q1759.6 209.115 1759.6 207.24 Q1759.6 205.504 1758.42 204.578 Q1757.26 203.652 1753.31 202.796 L1751.82 202.448 Q1747.87 201.615 1746.11 199.902 Q1744.35 198.166 1744.35 195.157 Q1744.35 191.499 1746.94 189.509 Q1749.53 187.518 1754.3 187.518 Q1756.66 187.518 1758.75 187.865 Q1760.83 188.212 1762.59 188.907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1792.94 200.041 L1792.94 202.124 L1773.35 202.124 Q1773.63 206.522 1775.99 208.837 Q1778.37 211.129 1782.61 211.129 Q1785.06 211.129 1787.36 210.527 Q1789.67 209.925 1791.94 208.721 L1791.94 212.749 Q1789.65 213.721 1787.24 214.231 Q1784.83 214.74 1782.36 214.74 Q1776.15 214.74 1772.52 211.129 Q1768.91 207.518 1768.91 201.36 Q1768.91 194.995 1772.33 191.268 Q1775.78 187.518 1781.62 187.518 Q1786.85 187.518 1789.88 190.897 Q1792.94 194.254 1792.94 200.041 M1788.68 198.791 Q1788.63 195.296 1786.71 193.212 Q1784.81 191.129 1781.66 191.129 Q1778.1 191.129 1775.94 193.143 Q1773.81 195.157 1773.49 198.814 L1788.68 198.791 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1799.93 178.05 L1804.18 178.05 L1804.18 214.069 L1799.93 214.069 L1799.93 178.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1835.27 200.041 L1835.27 202.124 L1815.69 202.124 Q1815.97 206.522 1818.33 208.837 Q1820.71 211.129 1824.95 211.129 Q1827.4 211.129 1829.69 210.527 Q1832.01 209.925 1834.28 208.721 L1834.28 212.749 Q1831.99 213.721 1829.58 214.231 Q1827.17 214.74 1824.69 214.74 Q1818.49 214.74 1814.86 211.129 Q1811.25 207.518 1811.25 201.36 Q1811.25 194.995 1814.67 191.268 Q1818.12 187.518 1823.95 187.518 Q1829.18 187.518 1832.22 190.897 Q1835.27 194.254 1835.27 200.041 M1831.01 198.791 Q1830.97 195.296 1829.05 193.212 Q1827.15 191.129 1824 191.129 Q1820.43 191.129 1818.28 193.143 Q1816.15 195.157 1815.83 198.814 L1831.01 198.791 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1860.92 189.138 L1860.92 193.12 Q1859.12 192.124 1857.29 191.638 Q1855.48 191.129 1853.63 191.129 Q1849.49 191.129 1847.19 193.768 Q1844.9 196.384 1844.9 201.129 Q1844.9 205.874 1847.19 208.513 Q1849.49 211.129 1853.63 211.129 Q1855.48 211.129 1857.29 210.643 Q1859.12 210.133 1860.92 209.138 L1860.92 213.073 Q1859.14 213.907 1857.22 214.323 Q1855.32 214.74 1853.17 214.74 Q1847.31 214.74 1843.86 211.059 Q1840.41 207.379 1840.41 201.129 Q1840.41 194.786 1843.88 191.152 Q1847.38 187.518 1853.44 187.518 Q1855.41 187.518 1857.29 187.935 Q1859.16 188.328 1860.92 189.138 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1872.54 180.782 L1872.54 188.143 L1881.31 188.143 L1881.31 191.453 L1872.54 191.453 L1872.54 205.527 Q1872.54 208.698 1873.4 209.601 Q1874.28 210.504 1876.94 210.504 L1881.31 210.504 L1881.31 214.069 L1876.94 214.069 Q1872.01 214.069 1870.13 212.24 Q1868.26 210.388 1868.26 205.527 L1868.26 191.453 L1865.13 191.453 L1865.13 188.143 L1868.26 188.143 L1868.26 180.782 L1872.54 180.782 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1887.1 179.509 L1894.07 179.509 L1902.89 203.027 L1911.75 179.509 L1918.72 179.509 L1918.72 214.069 L1914.16 214.069 L1914.16 183.722 L1905.25 207.425 L1900.55 207.425 L1891.64 183.722 L1891.64 214.069 L1887.1 214.069 L1887.1 179.509 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1939.6 201.036 Q1934.44 201.036 1932.45 202.217 Q1930.46 203.397 1930.46 206.245 Q1930.46 208.513 1931.94 209.856 Q1933.44 211.175 1936.01 211.175 Q1939.55 211.175 1941.68 208.675 Q1943.84 206.152 1943.84 201.985 L1943.84 201.036 L1939.6 201.036 M1948.1 199.277 L1948.1 214.069 L1943.84 214.069 L1943.84 210.133 Q1942.38 212.495 1940.2 213.629 Q1938.03 214.74 1934.88 214.74 Q1930.9 214.74 1928.54 212.518 Q1926.2 210.272 1926.2 206.522 Q1926.2 202.147 1929.11 199.925 Q1932.05 197.703 1937.86 197.703 L1943.84 197.703 L1943.84 197.286 Q1943.84 194.347 1941.89 192.749 Q1939.97 191.129 1936.48 191.129 Q1934.25 191.129 1932.15 191.661 Q1930.04 192.194 1928.1 193.259 L1928.1 189.323 Q1930.43 188.421 1932.63 187.981 Q1934.83 187.518 1936.92 187.518 Q1942.54 187.518 1945.32 190.434 Q1948.1 193.351 1948.1 199.277 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1978.42 188.143 L1969.05 200.759 L1978.91 214.069 L1973.88 214.069 L1966.34 203.883 L1958.79 214.069 L1953.77 214.069 L1963.84 200.504 L1954.62 188.143 L1959.65 188.143 L1966.52 197.379 L1973.4 188.143 L1978.42 188.143 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1984.25 178.097 L1987.96 178.097 Q1991.43 183.56 1993.14 188.791 Q1994.88 194.022 1994.88 199.184 Q1994.88 204.37 1993.14 209.624 Q1991.43 214.879 1987.96 220.319 L1984.25 220.319 Q1987.33 215.018 1988.84 209.786 Q1990.36 204.532 1990.36 199.184 Q1990.36 193.837 1988.84 188.629 Q1987.33 183.421 1984.25 178.097 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip940)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1225.73,248.629 1368.42,248.629 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M1396.87 249.404 L1396.87 262.066 L1404.37 262.066 Q1408.15 262.066 1409.95 260.515 Q1411.78 258.941 1411.78 255.723 Q1411.78 252.483 1409.95 250.955 Q1408.15 249.404 1404.37 249.404 L1396.87 249.404 M1396.87 235.191 L1396.87 245.608 L1403.79 245.608 Q1407.22 245.608 1408.89 244.335 Q1410.58 243.038 1410.58 240.399 Q1410.58 237.784 1408.89 236.487 Q1407.22 235.191 1403.79 235.191 L1396.87 235.191 M1392.2 231.349 L1404.14 231.349 Q1409.49 231.349 1412.38 233.571 Q1415.27 235.793 1415.27 239.89 Q1415.27 243.062 1413.79 244.937 Q1412.31 246.811 1409.44 247.274 Q1412.89 248.015 1414.79 250.376 Q1416.71 252.714 1416.71 256.233 Q1416.71 260.862 1413.56 263.385 Q1410.41 265.909 1404.6 265.909 L1392.2 265.909 L1392.2 231.349 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1436.32 252.876 Q1431.15 252.876 1429.16 254.057 Q1427.17 255.237 1427.17 258.085 Q1427.17 260.353 1428.65 261.696 Q1430.16 263.015 1432.73 263.015 Q1436.27 263.015 1438.4 260.515 Q1440.55 257.992 1440.55 253.825 L1440.55 252.876 L1436.32 252.876 M1444.81 251.117 L1444.81 265.909 L1440.55 265.909 L1440.55 261.973 Q1439.09 264.335 1436.92 265.469 Q1434.74 266.58 1431.59 266.58 Q1427.61 266.58 1425.25 264.358 Q1422.91 262.112 1422.91 258.362 Q1422.91 253.987 1425.83 251.765 Q1428.77 249.543 1434.58 249.543 L1440.55 249.543 L1440.55 249.126 Q1440.55 246.187 1438.61 244.589 Q1436.69 242.969 1433.19 242.969 Q1430.97 242.969 1428.86 243.501 Q1426.76 244.034 1424.81 245.099 L1424.81 241.163 Q1427.15 240.261 1429.35 239.821 Q1431.55 239.358 1433.63 239.358 Q1439.26 239.358 1442.03 242.274 Q1444.81 245.191 1444.81 251.117 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1470.11 240.747 L1470.11 244.774 Q1468.31 243.849 1466.36 243.386 Q1464.42 242.923 1462.33 242.923 Q1459.16 242.923 1457.57 243.895 Q1455.99 244.867 1455.99 246.811 Q1455.99 248.293 1457.13 249.149 Q1458.26 249.983 1461.69 250.747 L1463.15 251.071 Q1467.68 252.043 1469.58 253.825 Q1471.5 255.585 1471.5 258.756 Q1471.5 262.367 1468.63 264.473 Q1465.78 266.58 1460.78 266.58 Q1458.7 266.58 1456.43 266.163 Q1454.19 265.77 1451.69 264.96 L1451.69 260.561 Q1454.05 261.788 1456.34 262.413 Q1458.63 263.015 1460.88 263.015 Q1463.89 263.015 1465.51 261.997 Q1467.13 260.955 1467.13 259.08 Q1467.13 257.344 1465.95 256.418 Q1464.79 255.492 1460.83 254.636 L1459.35 254.288 Q1455.39 253.455 1453.63 251.742 Q1451.87 250.006 1451.87 246.997 Q1451.87 243.339 1454.46 241.349 Q1457.06 239.358 1461.83 239.358 Q1464.19 239.358 1466.27 239.705 Q1468.35 240.052 1470.11 240.747 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1478.28 239.983 L1482.54 239.983 L1482.54 265.909 L1478.28 265.909 L1478.28 239.983 M1478.28 229.89 L1482.54 229.89 L1482.54 235.284 L1478.28 235.284 L1478.28 229.89 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1510.11 240.978 L1510.11 244.96 Q1508.31 243.964 1506.48 243.478 Q1504.67 242.969 1502.82 242.969 Q1498.68 242.969 1496.39 245.608 Q1494.09 248.224 1494.09 252.969 Q1494.09 257.714 1496.39 260.353 Q1498.68 262.969 1502.82 262.969 Q1504.67 262.969 1506.48 262.483 Q1508.31 261.973 1510.11 260.978 L1510.11 264.913 Q1508.33 265.747 1506.41 266.163 Q1504.51 266.58 1502.36 266.58 Q1496.5 266.58 1493.05 262.899 Q1489.6 259.219 1489.6 252.969 Q1489.6 246.626 1493.08 242.992 Q1496.57 239.358 1502.64 239.358 Q1504.6 239.358 1506.48 239.775 Q1508.35 240.168 1510.11 240.978 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1517.71 231.349 L1522.38 231.349 L1522.38 245.515 L1539.37 245.515 L1539.37 231.349 L1544.05 231.349 L1544.05 265.909 L1539.37 265.909 L1539.37 249.45 L1522.38 249.45 L1522.38 265.909 L1517.71 265.909 L1517.71 231.349 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1575.34 251.881 L1575.34 253.964 L1555.76 253.964 Q1556.04 258.362 1558.4 260.677 Q1560.78 262.969 1565.02 262.969 Q1567.47 262.969 1569.76 262.367 Q1572.08 261.765 1574.35 260.561 L1574.35 264.589 Q1572.06 265.561 1569.65 266.071 Q1567.24 266.58 1564.76 266.58 Q1558.56 266.58 1554.93 262.969 Q1551.32 259.358 1551.32 253.2 Q1551.32 246.835 1554.74 243.108 Q1558.19 239.358 1564.02 239.358 Q1569.26 239.358 1572.29 242.737 Q1575.34 246.094 1575.34 251.881 M1571.08 250.631 Q1571.04 247.136 1569.12 245.052 Q1567.22 242.969 1564.07 242.969 Q1560.51 242.969 1558.35 244.983 Q1556.22 246.997 1555.9 250.654 L1571.08 250.631 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1581.89 255.677 L1581.89 239.983 L1586.15 239.983 L1586.15 255.515 Q1586.15 259.196 1587.59 261.048 Q1589.02 262.876 1591.89 262.876 Q1595.34 262.876 1597.33 260.677 Q1599.35 258.478 1599.35 254.682 L1599.35 239.983 L1603.61 239.983 L1603.61 265.909 L1599.35 265.909 L1599.35 261.927 Q1597.8 264.288 1595.74 265.446 Q1593.7 266.58 1590.99 266.58 Q1586.52 266.58 1584.21 263.802 Q1581.89 261.024 1581.89 255.677 M1592.61 239.358 L1592.61 239.358 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1627.4 243.964 Q1626.69 243.548 1625.83 243.362 Q1625 243.154 1623.98 243.154 Q1620.37 243.154 1618.42 245.515 Q1616.5 247.853 1616.5 252.251 L1616.5 265.909 L1612.22 265.909 L1612.22 239.983 L1616.5 239.983 L1616.5 244.011 Q1617.84 241.649 1620 240.515 Q1622.15 239.358 1625.23 239.358 Q1625.67 239.358 1626.2 239.427 Q1626.73 239.474 1627.38 239.589 L1627.4 243.964 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1631.87 239.983 L1636.13 239.983 L1636.13 265.909 L1631.87 265.909 L1631.87 239.983 M1631.87 229.89 L1636.13 229.89 L1636.13 235.284 L1631.87 235.284 L1631.87 229.89 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1661.57 240.747 L1661.57 244.774 Q1659.76 243.849 1657.82 243.386 Q1655.88 242.923 1653.79 242.923 Q1650.62 242.923 1649.02 243.895 Q1647.45 244.867 1647.45 246.811 Q1647.45 248.293 1648.58 249.149 Q1649.72 249.983 1653.14 250.747 L1654.6 251.071 Q1659.14 252.043 1661.04 253.825 Q1662.96 255.585 1662.96 258.756 Q1662.96 262.367 1660.09 264.473 Q1657.24 266.58 1652.24 266.58 Q1650.16 266.58 1647.89 266.163 Q1645.64 265.77 1643.14 264.96 L1643.14 260.561 Q1645.51 261.788 1647.8 262.413 Q1650.09 263.015 1652.33 263.015 Q1655.34 263.015 1656.96 261.997 Q1658.58 260.955 1658.58 259.08 Q1658.58 257.344 1657.4 256.418 Q1656.25 255.492 1652.29 254.636 L1650.81 254.288 Q1646.85 253.455 1645.09 251.742 Q1643.33 250.006 1643.33 246.997 Q1643.33 243.339 1645.92 241.349 Q1648.51 239.358 1653.28 239.358 Q1655.64 239.358 1657.73 239.705 Q1659.81 240.052 1661.57 240.747 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1673.95 232.622 L1673.95 239.983 L1682.73 239.983 L1682.73 243.293 L1673.95 243.293 L1673.95 257.367 Q1673.95 260.538 1674.81 261.441 Q1675.69 262.344 1678.35 262.344 L1682.73 262.344 L1682.73 265.909 L1678.35 265.909 Q1673.42 265.909 1671.55 264.08 Q1669.67 262.228 1669.67 257.367 L1669.67 243.293 L1666.55 243.293 L1666.55 239.983 L1669.67 239.983 L1669.67 232.622 L1673.95 232.622 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1688.33 239.983 L1692.59 239.983 L1692.59 265.909 L1688.33 265.909 L1688.33 239.983 M1688.33 229.89 L1692.59 229.89 L1692.59 235.284 L1688.33 235.284 L1688.33 229.89 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1720.16 240.978 L1720.16 244.96 Q1718.35 243.964 1716.52 243.478 Q1714.72 242.969 1712.87 242.969 Q1708.72 242.969 1706.43 245.608 Q1704.14 248.224 1704.14 252.969 Q1704.14 257.714 1706.43 260.353 Q1708.72 262.969 1712.87 262.969 Q1714.72 262.969 1716.52 262.483 Q1718.35 261.973 1720.16 260.978 L1720.16 264.913 Q1718.38 265.747 1716.45 266.163 Q1714.56 266.58 1712.4 266.58 Q1706.55 266.58 1703.1 262.899 Q1699.65 259.219 1699.65 252.969 Q1699.65 246.626 1703.12 242.992 Q1706.62 239.358 1712.68 239.358 Q1714.65 239.358 1716.52 239.775 Q1718.4 240.168 1720.16 240.978 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1737.8 229.937 Q1734.69 235.261 1733.19 240.469 Q1731.69 245.677 1731.69 251.024 Q1731.69 256.372 1733.19 261.626 Q1734.72 266.858 1737.8 272.159 L1734.09 272.159 Q1730.62 266.719 1728.88 261.464 Q1727.17 256.21 1727.17 251.024 Q1727.17 245.862 1728.88 240.631 Q1730.6 235.4 1734.09 229.937 L1737.8 229.937 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1762.59 240.747 L1762.59 244.774 Q1760.78 243.849 1758.84 243.386 Q1756.89 242.923 1754.81 242.923 Q1751.64 242.923 1750.04 243.895 Q1748.47 244.867 1748.47 246.811 Q1748.47 248.293 1749.6 249.149 Q1750.74 249.983 1754.16 250.747 L1755.62 251.071 Q1760.16 252.043 1762.06 253.825 Q1763.98 255.585 1763.98 258.756 Q1763.98 262.367 1761.11 264.473 Q1758.26 266.58 1753.26 266.58 Q1751.18 266.58 1748.91 266.163 Q1746.66 265.77 1744.16 264.96 L1744.16 260.561 Q1746.52 261.788 1748.81 262.413 Q1751.11 263.015 1753.35 263.015 Q1756.36 263.015 1757.98 261.997 Q1759.6 260.955 1759.6 259.08 Q1759.6 257.344 1758.42 256.418 Q1757.26 255.492 1753.31 254.636 L1751.82 254.288 Q1747.87 253.455 1746.11 251.742 Q1744.35 250.006 1744.35 246.997 Q1744.35 243.339 1746.94 241.349 Q1749.53 239.358 1754.3 239.358 Q1756.66 239.358 1758.75 239.705 Q1760.83 240.052 1762.59 240.747 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1792.94 251.881 L1792.94 253.964 L1773.35 253.964 Q1773.63 258.362 1775.99 260.677 Q1778.37 262.969 1782.61 262.969 Q1785.06 262.969 1787.36 262.367 Q1789.67 261.765 1791.94 260.561 L1791.94 264.589 Q1789.65 265.561 1787.24 266.071 Q1784.83 266.58 1782.36 266.58 Q1776.15 266.58 1772.52 262.969 Q1768.91 259.358 1768.91 253.2 Q1768.91 246.835 1772.33 243.108 Q1775.78 239.358 1781.62 239.358 Q1786.85 239.358 1789.88 242.737 Q1792.94 246.094 1792.94 251.881 M1788.68 250.631 Q1788.63 247.136 1786.71 245.052 Q1784.81 242.969 1781.66 242.969 Q1778.1 242.969 1775.94 244.983 Q1773.81 246.997 1773.49 250.654 L1788.68 250.631 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1799.93 229.89 L1804.18 229.89 L1804.18 265.909 L1799.93 265.909 L1799.93 229.89 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1835.27 251.881 L1835.27 253.964 L1815.69 253.964 Q1815.97 258.362 1818.33 260.677 Q1820.71 262.969 1824.95 262.969 Q1827.4 262.969 1829.69 262.367 Q1832.01 261.765 1834.28 260.561 L1834.28 264.589 Q1831.99 265.561 1829.58 266.071 Q1827.17 266.58 1824.69 266.58 Q1818.49 266.58 1814.86 262.969 Q1811.25 259.358 1811.25 253.2 Q1811.25 246.835 1814.67 243.108 Q1818.12 239.358 1823.95 239.358 Q1829.18 239.358 1832.22 242.737 Q1835.27 246.094 1835.27 251.881 M1831.01 250.631 Q1830.97 247.136 1829.05 245.052 Q1827.15 242.969 1824 242.969 Q1820.43 242.969 1818.28 244.983 Q1816.15 246.997 1815.83 250.654 L1831.01 250.631 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1860.92 240.978 L1860.92 244.96 Q1859.12 243.964 1857.29 243.478 Q1855.48 242.969 1853.63 242.969 Q1849.49 242.969 1847.19 245.608 Q1844.9 248.224 1844.9 252.969 Q1844.9 257.714 1847.19 260.353 Q1849.49 262.969 1853.63 262.969 Q1855.48 262.969 1857.29 262.483 Q1859.12 261.973 1860.92 260.978 L1860.92 264.913 Q1859.14 265.747 1857.22 266.163 Q1855.32 266.58 1853.17 266.58 Q1847.31 266.58 1843.86 262.899 Q1840.41 259.219 1840.41 252.969 Q1840.41 246.626 1843.88 242.992 Q1847.38 239.358 1853.44 239.358 Q1855.41 239.358 1857.29 239.775 Q1859.16 240.168 1860.92 240.978 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1872.54 232.622 L1872.54 239.983 L1881.31 239.983 L1881.31 243.293 L1872.54 243.293 L1872.54 257.367 Q1872.54 260.538 1873.4 261.441 Q1874.28 262.344 1876.94 262.344 L1881.31 262.344 L1881.31 265.909 L1876.94 265.909 Q1872.01 265.909 1870.13 264.08 Q1868.26 262.228 1868.26 257.367 L1868.26 243.293 L1865.13 243.293 L1865.13 239.983 L1868.26 239.983 L1868.26 232.622 L1872.54 232.622 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1906.61 273.779 L1906.61 277.089 L1881.99 277.089 L1881.99 273.779 L1906.61 273.779 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1925.64 243.964 Q1924.93 243.548 1924.07 243.362 Q1923.24 243.154 1922.22 243.154 Q1918.61 243.154 1916.66 245.515 Q1914.74 247.853 1914.74 252.251 L1914.74 265.909 L1910.46 265.909 L1910.46 239.983 L1914.74 239.983 L1914.74 244.011 Q1916.08 241.649 1918.24 240.515 Q1920.39 239.358 1923.47 239.358 Q1923.91 239.358 1924.44 239.427 Q1924.97 239.474 1925.62 239.589 L1925.64 243.964 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1941.89 252.876 Q1936.73 252.876 1934.74 254.057 Q1932.75 255.237 1932.75 258.085 Q1932.75 260.353 1934.23 261.696 Q1935.74 263.015 1938.3 263.015 Q1941.85 263.015 1943.98 260.515 Q1946.13 257.992 1946.13 253.825 L1946.13 252.876 L1941.89 252.876 M1950.39 251.117 L1950.39 265.909 L1946.13 265.909 L1946.13 261.973 Q1944.67 264.335 1942.49 265.469 Q1940.32 266.58 1937.17 266.58 Q1933.19 266.58 1930.83 264.358 Q1928.49 262.112 1928.49 258.362 Q1928.49 253.987 1931.41 251.765 Q1934.35 249.543 1940.16 249.543 L1946.13 249.543 L1946.13 249.126 Q1946.13 246.187 1944.18 244.589 Q1942.26 242.969 1938.77 242.969 Q1936.55 242.969 1934.44 243.501 Q1932.33 244.034 1930.39 245.099 L1930.39 241.163 Q1932.73 240.261 1934.92 239.821 Q1937.12 239.358 1939.21 239.358 Q1944.83 239.358 1947.61 242.274 Q1950.39 245.191 1950.39 251.117 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1980.71 250.261 L1980.71 265.909 L1976.45 265.909 L1976.45 250.399 Q1976.45 246.719 1975.02 244.89 Q1973.58 243.062 1970.71 243.062 Q1967.26 243.062 1965.27 245.261 Q1963.28 247.46 1963.28 251.256 L1963.28 265.909 L1959 265.909 L1959 239.983 L1963.28 239.983 L1963.28 244.011 Q1964.81 241.673 1966.87 240.515 Q1968.95 239.358 1971.66 239.358 Q1976.13 239.358 1978.42 242.136 Q1980.71 244.89 1980.71 250.261 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2006.27 243.918 L2006.27 229.89 L2010.53 229.89 L2010.53 265.909 L2006.27 265.909 L2006.27 262.02 Q2004.92 264.335 2002.86 265.469 Q2000.83 266.58 1997.96 266.58 Q1993.26 266.58 1990.3 262.83 Q1987.36 259.08 1987.36 252.969 Q1987.36 246.858 1990.3 243.108 Q1993.26 239.358 1997.96 239.358 Q2000.83 239.358 2002.86 240.492 Q2004.92 241.603 2006.27 243.918 M1991.75 252.969 Q1991.75 257.668 1993.67 260.353 Q1995.62 263.015 1999 263.015 Q2002.38 263.015 2004.32 260.353 Q2006.27 257.668 2006.27 252.969 Q2006.27 248.27 2004.32 245.608 Q2002.38 242.923 1999 242.923 Q1995.62 242.923 1993.67 245.608 Q1991.75 248.27 1991.75 252.969 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2029.35 242.969 Q2025.92 242.969 2023.93 245.654 Q2021.94 248.316 2021.94 252.969 Q2021.94 257.622 2023.91 260.307 Q2025.9 262.969 2029.35 262.969 Q2032.75 262.969 2034.74 260.284 Q2036.73 257.598 2036.73 252.969 Q2036.73 248.362 2034.74 245.677 Q2032.75 242.969 2029.35 242.969 M2029.35 239.358 Q2034.9 239.358 2038.07 242.969 Q2041.24 246.58 2041.24 252.969 Q2041.24 259.335 2038.07 262.969 Q2034.9 266.58 2029.35 266.58 Q2023.77 266.58 2020.6 262.969 Q2017.45 259.335 2017.45 252.969 Q2017.45 246.58 2020.6 242.969 Q2023.77 239.358 2029.35 239.358 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2068.49 244.96 Q2070.09 242.089 2072.31 240.724 Q2074.53 239.358 2077.54 239.358 Q2081.59 239.358 2083.79 242.205 Q2085.99 245.029 2085.99 250.261 L2085.99 265.909 L2081.71 265.909 L2081.71 250.399 Q2081.71 246.673 2080.39 244.867 Q2079.07 243.062 2076.36 243.062 Q2073.05 243.062 2071.13 245.261 Q2069.21 247.46 2069.21 251.256 L2069.21 265.909 L2064.92 265.909 L2064.92 250.399 Q2064.92 246.649 2063.6 244.867 Q2062.29 243.062 2059.53 243.062 Q2056.27 243.062 2054.35 245.284 Q2052.42 247.483 2052.42 251.256 L2052.42 265.909 L2048.14 265.909 L2048.14 239.983 L2052.42 239.983 L2052.42 244.011 Q2053.88 241.626 2055.92 240.492 Q2057.96 239.358 2060.76 239.358 Q2063.58 239.358 2065.55 240.793 Q2067.54 242.228 2068.49 244.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2114.18 273.779 L2114.18 277.089 L2089.55 277.089 L2089.55 273.779 L2114.18 273.779 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2115.13 239.983 L2119.65 239.983 L2127.75 261.742 L2135.85 239.983 L2140.36 239.983 L2130.64 265.909 L2124.85 265.909 L2115.13 239.983 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2158.03 252.876 Q2152.86 252.876 2150.87 254.057 Q2148.88 255.237 2148.88 258.085 Q2148.88 260.353 2150.36 261.696 Q2151.87 263.015 2154.44 263.015 Q2157.98 263.015 2160.11 260.515 Q2162.26 257.992 2162.26 253.825 L2162.26 252.876 L2158.03 252.876 M2166.52 251.117 L2166.52 265.909 L2162.26 265.909 L2162.26 261.973 Q2160.8 264.335 2158.63 265.469 Q2156.45 266.58 2153.3 266.58 Q2149.32 266.58 2146.96 264.358 Q2144.62 262.112 2144.62 258.362 Q2144.62 253.987 2147.54 251.765 Q2150.48 249.543 2156.29 249.543 L2162.26 249.543 L2162.26 249.126 Q2162.26 246.187 2160.32 244.589 Q2158.4 242.969 2154.9 242.969 Q2152.68 242.969 2150.57 243.501 Q2148.47 244.034 2146.52 245.099 L2146.52 241.163 Q2148.86 240.261 2151.06 239.821 Q2153.26 239.358 2155.34 239.358 Q2160.97 239.358 2163.74 242.274 Q2166.52 245.191 2166.52 251.117 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2175.29 229.89 L2179.55 229.89 L2179.55 265.909 L2175.29 265.909 L2175.29 229.89 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2188.03 255.677 L2188.03 239.983 L2192.28 239.983 L2192.28 255.515 Q2192.28 259.196 2193.72 261.048 Q2195.16 262.876 2198.03 262.876 Q2201.47 262.876 2203.47 260.677 Q2205.48 258.478 2205.48 254.682 L2205.48 239.983 L2209.74 239.983 L2209.74 265.909 L2205.48 265.909 L2205.48 261.927 Q2203.93 264.288 2201.87 265.446 Q2199.83 266.58 2197.12 266.58 Q2192.66 266.58 2190.34 263.802 Q2188.03 261.024 2188.03 255.677 M2198.74 239.358 L2198.74 239.358 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2240.69 251.881 L2240.69 253.964 L2221.1 253.964 Q2221.38 258.362 2223.74 260.677 Q2226.13 262.969 2230.36 262.969 Q2232.82 262.969 2235.11 262.367 Q2237.42 261.765 2239.69 260.561 L2239.69 264.589 Q2237.4 265.561 2234.99 266.071 Q2232.59 266.58 2230.11 266.58 Q2223.9 266.58 2220.27 262.969 Q2216.66 259.358 2216.66 253.2 Q2216.66 246.835 2220.09 243.108 Q2223.53 239.358 2229.37 239.358 Q2234.6 239.358 2237.63 242.737 Q2240.69 246.094 2240.69 251.881 M2236.43 250.631 Q2236.38 247.136 2234.46 245.052 Q2232.56 242.969 2229.41 242.969 Q2225.85 242.969 2223.7 244.983 Q2221.57 246.997 2221.24 250.654 L2236.43 250.631 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2247.01 229.937 L2250.71 229.937 Q2254.18 235.4 2255.9 240.631 Q2257.63 245.862 2257.63 251.024 Q2257.63 256.21 2255.9 261.464 Q2254.18 266.719 2250.71 272.159 L2247.01 272.159 Q2250.09 266.858 2251.59 261.626 Q2253.12 256.372 2253.12 251.024 Q2253.12 245.677 2251.59 240.469 Q2250.09 235.261 2247.01 229.937 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the evaluation_df to include only the SimpleLearnedHeuristic\n",
    "learned_heuristic_df = filter(row -> row[:heuristic_type] == \"SimpleLearnedHeuristic\", evaluation_df)\n",
    "\n",
    "# Filter the evaluation_df to include only the BasicHeuristic(selectMax)\n",
    "select_max_df = filter(row -> row[:heuristic_type] == \"BasicHeuristic(selectMax)\", evaluation_df)\n",
    "\n",
    "# Filter the evaluation_df to include only the BasicHeuristic(select_random_value)\n",
    "random_heuristic_df = filter(row -> row[:heuristic_type] == \"BasicHeuristic(select_random_value)\", evaluation_df)\n",
    "\n",
    "# Create a line plot of the first solution for each instance for all heuristics\n",
    "plot()\n",
    "plot!(learned_heuristic_df[!, :num_instance], learned_heuristic_df[!, :first_sol], label=\"SimpleLearnedHeuristic\", linewidth=2)\n",
    "plot!(select_max_df[!, :num_instance], select_max_df[!, :first_sol], label=\"BasicHeuristic(selectMax)\")\n",
    "plot!(random_heuristic_df[!, :num_instance], random_heuristic_df[!, :first_sol], label=\"BasicHeuristic(select_random_value)\")\n",
    "xlabel!(\"Instance Number\")\n",
    "ylabel!(\"First Solution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Large Problems with Pre-Trained Model\n",
    "\n",
    "The last example was on trivially small problems. Now let's have a look at the performance of learned heuristics on larger problems. For this section, we will load a model trained on MIS problems with 40 nodes and we will be comparing the performance of the learned heuristc with random heuristics and the select max heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//model_mis8.bson\n"
     ]
    }
   ],
   "source": [
    "using BSON: @save, @load\n",
    "include(\"../../../learning_cp/utils/benchmark.jl\")\n",
    "validation_generator = SeaPearl.MaximumIndependentSetGenerator(40, 4)\n",
    "\n",
    "num_instances = 20 # Number of instances to evaluate on\n",
    "node_budget = 1000 # Budget of visited nodes\n",
    "take_objective = false # Set it to true if we have to branch on the object ive variable\n",
    "eval_strategy = SeaPearl.DFSearch()\n",
    "include_dfs = true # Set it to true if you want to evaluate with DFS in addition to ILDS\n",
    "basicHeuristics = Dict()\n",
    "num_random_heuristics = 2\n",
    "\n",
    "for (i, random_heuristic) in enumerate(randomHeuristics)\n",
    "    push!(basicHeuristics, \"random\" * string(i) => random_heuristic)\n",
    "end\n",
    "\n",
    "\n",
    "folder = \"./\"\n",
    "models = []\n",
    "model_names = []\n",
    "for file in readdir(folder)\n",
    "    if splitext(file)[2] == \".bson\"\n",
    "        println(folder * \"/\" * file)\n",
    "\n",
    "        @load folder * \"/\" * file model\n",
    "        push!(models, model)\n",
    "        push!(model_names, replace(splitext(file)[1], \"model_\" => \"\"))\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Any}:\n",
       " NeuralNetworkApproximator{SeaPearl.HeterogeneousFullFeaturedCPNN, ADAM}(SeaPearl.HeterogeneousFullFeaturedCPNN(HeterogeneousModel{SeaPearl.HeterogeneousGraphConvInit{Matrix{Float32}, Vector{Float32}}, SeaPearl.HeterogeneousGraphConv{Matrix{Float32}, Vector{Float32}, SeaPearl.meanPooling}}(SeaPearl.HeterogeneousGraphConvInit{Matrix{Float32}, Vector{Float32}}(Float32[0.40720457 -0.32606193 â€¦ 0.5077556 -0.50557137; -0.3021081 0.1177144 â€¦ 0.36330396 -0.20401394; â€¦ ; 0.5697128 -0.51533896 â€¦ -0.04572114 -0.47046334; 0.17841344 0.3554702 â€¦ -0.3777368 -0.07719718], Float32[-0.33453026 0.062046878 â€¦ 0.57219756 -0.59562236; 0.29490018 -0.12420616 â€¦ 0.19817024 -0.3452474; â€¦ ; -0.23823601 0.4619464 â€¦ -0.24487855 -0.35581324; 0.63930255 -0.14703989 â€¦ 0.33584824 0.62856567], Float32[-0.1483127 -0.6990236; 0.16246367 0.12005859; â€¦ ; -0.4109148 -0.044561937; -0.03639565 -0.37019196], Float32[-0.3115838, 0.49910548, 0.612982, -0.13171312, 0.43216503, 0.65782726, -0.37980285, 0.25139692], Float32[-0.12355303, -0.23362063, 0.2528947, 0.7158204, -0.159523, -0.54613787, 0.5618333, -0.34768632], Float32[-0.41794512, -0.21348865, 0.004955468, -0.7843505, 0.43665758, -0.033967223, 0.33494258, 0.059932034], NNlib.leakyrelu), SeaPearl.HeterogeneousGraphConv{Matrix{Float32}, Vector{Float32}, SeaPearl.meanPooling}[SeaPearl.HeterogeneousGraphConv{Matrix{Float32}, Vector{Float32}, SeaPearl.meanPooling}(Float32[0.20400971 0.289822 â€¦ -0.3245698 -0.026451873; -0.36162123 -0.2648165 â€¦ 0.27667627 0.124139376; â€¦ ; 0.055427376 0.05712347 â€¦ -0.2957347 -0.13767686; -0.03824277 0.36080384 â€¦ -0.3009362 0.24016398], Float32[-0.25252977 0.33328682 â€¦ -0.14709583 0.45165098; 0.063819855 -0.08975643 â€¦ -0.35611326 -0.19395092; â€¦ ; -0.2291572 -0.24726607 â€¦ -0.13474016 0.15989809; 0.17371345 -0.43942913 â€¦ -0.2253648 0.0019480299], Float32[-0.14400227 0.35780287 â€¦ 0.40035677 0.27790037; 0.23347375 -0.41487285 â€¦ 0.03867983 -0.45285496; â€¦ ; -0.16291103 0.36875334 â€¦ -0.010113227 0.06800143; 0.20516966 -0.2935854 â€¦ -0.26782048 0.0025045986], Float32[0.62761164, 0.6633463, 0.60059094, 0.6982697, -0.4037468, 0.546848, 0.19654806, -0.21278629], Float32[-0.30591485, 0.083874635, -0.62996227, -0.5468383, 0.52267355, -0.33248413, 0.43439785, 0.50753635], Float32[-0.52084017, -0.10243525, -0.24382143, -0.25582895, -0.47914386, 0.17176937, -0.36773205, 0.78236896], NNlib.leakyrelu, SeaPearl.meanPooling()), SeaPearl.HeterogeneousGraphConv{Matrix{Float32}, Vector{Float32}, SeaPearl.meanPooling}(Float32[0.30036917 -0.024314111 â€¦ 0.085729115 0.23802584; -0.12624748 -0.31170914 â€¦ 0.14453049 0.1666481; â€¦ ; 0.3769522 -0.3020594 â€¦ 0.36822248 0.2140815; -0.13030133 -0.047444664 â€¦ 0.27121812 -0.05458828], Float32[-0.2867365 0.026772913 â€¦ 0.27517128 0.104403466; 0.2459144 -0.0092546325 â€¦ -0.35298088 0.13132104; â€¦ ; 0.096204236 -0.23361935 â€¦ 0.13371079 -0.08075849; 0.3449798 0.10622147 â€¦ 0.27359036 0.3174253], Float32[0.38795748 0.32921326 â€¦ -0.31479394 -0.32274297; -0.07172443 0.4537134 â€¦ 0.035079267 0.32201844; â€¦ ; 0.21876445 -0.30558094 â€¦ -0.09572278 -0.11651732; -0.46246895 -0.3427856 â€¦ 0.33060035 -0.1927652], Float32[-0.21262822, -0.07833672, -0.80663824, -0.33146426, 0.33994418, 0.5759407, 0.7129066, 0.6127953], Float32[0.18432273, 0.36680564, -0.14633268, 0.7542494, -0.6633237, -0.21247151, 0.635799, 0.5208894], Float32[-0.30664662, -0.35726866, -0.6485714, -0.15541142, 0.5393755, -0.15407288, 0.11956428, -0.4189089], NNlib.leakyrelu, SeaPearl.meanPooling())]), Chain(Dense(8, 16, leakyrelu), Dense(16, 16, leakyrelu), Dense(16, 16)), Chain(Dense(8, 16, leakyrelu), Dense(16, 16, leakyrelu), Dense(16, 16)), Chain(), Chain(Dense(32, 16, leakyrelu), Dense(16, 1))), ADAM(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}(Float32[0.009564231, 0.22599985, -0.03370343, -0.012441907, -0.027061421, -0.026934186, -0.0016752975, 0.0679851, -0.117933825, -0.014591963, 0.018879483, -0.02593981, 0.0005638582, 0.018223204, 0.037359245, 0.013058252] => (Float32[-0.0007443341, -0.005370633, 0.0017289608, -0.0012381829, -7.513699f-5, -0.0011090268, -5.816243f-6, 2.1493799f-5, -0.00042159442, -0.0004098061, -1.7507793f-5, 1.777907f-5, -0.0006122896, 7.4267655f-6, -0.0015389572, 0.00073930284], Float32[2.122755f-5, 0.0004965801, 0.00013804996, 3.7885584f-5, 1.4853014f-5, 4.630552f-5, 1.3099376f-9, 6.744474f-8, 0.0003556681, 2.4009814f-5, 5.4626863f-9, 5.5416187f-9, 3.96065f-5, 1.6270522f-9, 2.6697135f-5, 1.3973425f-5], [2.479069494136023e-188, 0.016538713596848224]), Float32[-0.40019715 0.0090780165 â€¦ 0.20256148 -0.42948452; -0.041697502 0.31871128 â€¦ -0.32628384 -0.25223103; â€¦ ; -0.13304386 -0.316654 â€¦ -0.1528982 0.23006842; -0.36008832 -0.079930104 â€¦ 0.09574076 0.2651535] => (Float32[6.209078f-6 -0.00044744695 â€¦ -0.0002431417 7.4703726f-6; 4.6357996f-5 -0.0032422394 â€¦ -0.0022755947 5.958298f-5; â€¦ ; 1.4717952f-5 -0.001006057 â€¦ -0.00054583984 1.5897465f-5; -7.5263556f-6 0.00050212647 â€¦ 0.00018854585 -7.116388f-6], Float32[1.8196111f-9 6.8906393f-6 â€¦ 1.4811579f-5 3.4993894f-9; 4.225668f-8 0.00019674594 â€¦ 0.00017583843 8.260916f-8; â€¦ ; 1.9903958f-9 8.834585f-6 â€¦ 4.4324647f-6 2.7610443f-9; 1.6313992f-9 6.0466245f-6 â€¦ 1.1565494f-5 3.0689715f-9], [2.479069494136023e-188, 0.016538713596848224]), Float32[-0.04267671, 0.021346977, 0.038374677, -0.05418132, 0.026592333, -0.0077106496, -0.013144064, 0.018547244, -0.0339581, -0.0062168064, 0.005778656, 0.039638735, -0.038850643, 0.031334188, 0.022600694, 0.076496005] => (Float32[0.00044934353, -0.0009582567, -8.11811f-5, 0.0020656376, -0.00025701834, -0.0013410582, 0.0010839031, 0.00026515347, 0.0025597245, -3.0357949f-5, -0.00088964216, 0.00015815262, 0.00067944376, -0.0012959185, -0.0008562587, 0.00038113954], Float32[7.094003f-5, 1.8800074f-5, 1.090193f-5, 0.00012685376, 7.4437276f-6, 3.2385575f-5, 2.615505f-5, 2.4858651f-5, 0.00015461286, 1.9807578f-5, 6.5021804f-6, 7.7164605f-6, 0.00011186138, 3.6805774f-5, 3.305925f-5, 2.584982f-6], [2.479069494136023e-188, 0.016538713596848224]), Float32[0.053939816, -0.013828802, -0.060304344, 0.05472456, 0.08232005, -0.020134965, -0.020208212, -0.015317106, -0.054284573, -0.017210422, 0.054020237, -0.023638004, -0.073488094, -0.018473733, -0.053973008, -0.05437194] => (Float32[-0.0013416855, 2.399259f-5, 1.3378547f-6, -0.00069152593, -0.0011076822, 2.7366408f-5, 2.2130045f-5, 3.075773f-5, 0.0021437577, 1.617901f-5, -1.8835888f-5, 0.003318113, 0.0009805369, 2.6874133f-5, 3.3506396f-5, 5.556728f-6], Float32[4.544188f-5, 4.2910397f-6, 3.3551318f-11, 1.2455124f-5, 2.4234068f-5, 3.195108f-5, 2.0642896f-5, 2.3365506f-5, 0.0001161327, 9.605054f-9, 8.984017f-9, 0.00011617683, 0.00010074909, 2.1313062f-5, 2.848887f-8, 8.692121f-10], [2.479069494136023e-188, 0.016538713596848224]), Float32[0.2437259 -0.43480054 â€¦ -0.6076622 -0.101297066] => (Float32[-0.004997814 5.8221405f-5 â€¦ 3.3029148f-5 3.497566f-5], Float32[0.00065215037 1.9212737f-6 â€¦ 3.1374327f-8 4.0125514f-8], [2.479069494136023e-188, 0.016538713596848224]), Float32[-0.37580374 -0.014592632 â€¦ 0.19981298 -0.43604797; 0.0074926843 0.14567238 â€¦ -0.0981353 -0.20105545; â€¦ ; -0.087604664 -0.36133295 â€¦ -0.026742347 0.17140996; -0.40114114 -0.12643678 â€¦ 0.19894883 0.31206703] => (Float32[-6.161431f-8 7.043394f-6 â€¦ -5.825192f-8 9.772626f-6; 1.14346804f-7 -1.1567004f-5 â€¦ 7.31639f-8 -1.7396014f-5; â€¦ ; 4.0310756f-6 -0.00035894933 â€¦ 1.3759179f-6 -0.0005779144; 7.172412f-6 -0.00071850396 â€¦ 4.330497f-6 -0.0010714215], Float32[1.9055039f-11 7.7495605f-7 â€¦ 6.4921575f-11 8.695272f-7; 2.1415655f-13 3.1564051f-9 â€¦ 2.7909657f-13 5.767838f-9; â€¦ ; 4.0296405f-10 5.5605383f-6 â€¦ 4.4668394f-10 1.0564298f-5; 7.351797f-10 1.1029956f-5 â€¦ 9.167539f-10 2.0085232f-5], [2.479069494136023e-188, 0.016538713596848224]), Float32[-0.010566905, 0.01175855, 0.010757135, -0.013955792, -0.016944388, -0.024204446, 0.032424457, 0.004825784, 0.04254383, -0.058496606, -0.004474846, -0.010396028, -0.008662246, -0.02182641, 0.025918385, 0.0018356813] => (Float32[1.1512131f-5, -1.9967509f-5, -0.0008525662, 0.0011987626, -0.0010857377, 0.0005552404, -3.1293435f-5, 1.34658185f-5, -2.6558222f-5, 6.4207043f-6, -0.00033448078, 0.00356299, 0.001333745, 1.8758497f-5, -0.00066758733, -0.0012567719], Float32[1.0423356f-6, 5.3026907f-9, 1.5026463f-5, 2.9009747f-5, 8.671372f-6, 1.4570166f-5, 8.774146f-9, 1.809206f-9, 5.762819f-9, 2.6162335f-9, 1.207487f-5, 0.00012168985, 1.788669f-5, 2.2016628f-5, 9.338711f-6, 1.9072544f-5], [2.479069494136023e-188, 0.016538713596848224]), Float32[0.053930942] => (Float32[-0.005519948], Float32[0.00078730774], [2.479069494136023e-188, 0.016538713596848224]), Float32[-0.026588716, -0.023824176, -0.0593965, 0.008560458, -0.023832541, 0.013086449, -0.0063636433, 0.012801605, 0.028683973, 0.0050036097, 0.011157974, -0.009898751, -0.043581516, -0.0018060151, 0.038249783, 0.008818522] => (Float32[1.0824526f-5, 0.0006262458, 0.001948068, -0.00048827654, -2.0471284f-6, -0.0014641744, -9.801421f-6, -0.0002032028, -6.7843166f-7, -1.7475346f-5, -1.5306348f-5, 0.00040437718, 0.0017611933, -0.00029050885, -2.2543316f-5, -0.001249017], Float32[1.010894f-9, 4.664777f-6, 3.7409656f-5, 1.8709463f-6, 1.1477042f-9, 2.3811841f-5, 3.2021432f-9, 1.7407186f-6, 1.255599f-10, 6.509676f-9, 2.556036f-9, 1.289568f-5, 2.5790638f-5, 1.5399779f-5, 3.1777594f-9, 1.8463687f-5], [2.479069494136023e-188, 0.016538713596848224]), Float32[-0.40803984 -0.18130368 â€¦ 0.09781893 0.20945007; 0.2520997 0.24297012 â€¦ 0.2606779 -0.25243962; â€¦ ; -0.28205585 -0.44139072 â€¦ -0.13734746 0.07592124; 0.39991286 -0.079352506 â€¦ -0.12033577 0.11106268] => (Float32[-2.0899644f-5 6.085642f-8 â€¦ -8.933434f-6 -6.018345f-8; -0.0012816038 8.038312f-6 â€¦ -0.00086502143 5.5124597f-6; â€¦ ; 0.005347379 -2.4909075f-5 â€¦ 0.0030554153 -1.0577076f-5; 4.4536526f-5 -2.063993f-7 â€¦ 2.5684589f-5 -4.9443436f-8], Float32[1.1268725f-7 2.2274186f-12 â€¦ 2.2442972f-8 5.8331248f-12; 0.0002973836 9.2542844f-9 â€¦ 9.741204f-5 7.523337f-9; â€¦ ; 0.0006749695 1.8181167f-8 â€¦ 0.00021361295 1.6483572f-8; 1.05072914f-7 2.7687728f-12 â€¦ 3.1667987f-8 2.3193875f-12], [2.479069494136023e-188, 0.016538713596848224])â€¦)))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

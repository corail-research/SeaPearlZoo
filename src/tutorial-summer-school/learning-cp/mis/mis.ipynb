{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS\n",
    "In a graph, the maximal independent set, also known as maximal stable set is an independent set that is not a subset of any other independent set. In other words, it is a set of vertices such that no two vertices in the set are adjacent. In this notebook, we will build a learning-based model to find the maximal independent set in a graph.\n",
    "\n",
    "<img src=\"img/Independent_set_graph.png\" alt=\"Solved MIS\" style=\"width:400px; height:400px;\">\n",
    "\n",
    "SeaPearl currently supports learning-based value selection models trained with Reinforcement Learning. To train Reinforcement Learning agents, we start by generating training instances and we let the agent learn a value selection heuristic. Like all other Reinforcement Learning tasks, we need to define a reward function, a state representation and an action space. Finally, we also need to build a neural network that will learn the value selection heuristic. All of these components will be defined in the following sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will begin by activating the environment and importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Pkg\n",
    "Pkg.activate(\"../../../../\")\n",
    "Pkg.instantiate()\n",
    "using SeaPearl\n",
    "using Flux\n",
    "using LightGraphs\n",
    "using Random\n",
    "using BSON: @save, @load\n",
    "using ReinforcementLearning\n",
    "using CSV\n",
    "const RL = ReinforcementLearning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating instances\n",
    "\n",
    "SeaPearl provides a number of instance generators, including one for the MIS problem. Under the hood, this generator creates Barabasi-Albert graphs with `n` vertices. The graphs are grown by adding new vertices to an initial that has `k` vertices. New vertices are connected by `k` edges to `k` different vertices already present in the system by preferential attachment. The resulting graphs are undirected.\n",
    "\n",
    "<img src=\"img/450px-Barabasi_albert_graph.png\" alt=\"Barabasi-Albert Graph\" style=\"width:600px; height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.MaximumIndependentSetGenerator(8, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numInitialVertices = 3\n",
    "numNewVertices = 8\n",
    "instance_generator = SeaPearl.MaximumIndependentSetGenerator(numNewVertices, numInitialVertices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning setup\n",
    "\n",
    "### The state representation\n",
    "In SeaPearl, the state $s_t$ is defined as a pair $s_t = (P_t, x_t)$, with $P_t$ a partially solved combinatorial optimization problem and $x_t$ a variable selected at time $t$ of an episode. A terminal episode is reached if all variables are fixed or if a failure is detected.\n",
    "\n",
    "### The action space\n",
    "Given a state $s_t = (P_t, x_t)$, an action $a_t$ represents the selection of a value $v$ for the variable $x_t$. The action space is defined as the set of all possible values for the variable $x_t$ at time $t$.\n",
    "\n",
    "### The transition function\n",
    "Given a state $s_t = (P_t, x_t)$ and an action $a_t = v$, the transition function is comprised of three steps;\n",
    "1. The value of variable $x_{267}$ is assigned as $v$ (i.e., $D(x_{t+1}) = v$).\n",
    "2. The fix-point operation is applied on $P_t$ to prune the domains (i.e., $P_{t+1} = \\text{{fixPoint}}(P_t)$).\n",
    "3. The next variable to branch on is selected (i.e., $x_{t+1} = \\text{{nextVariable}}(P_{t+1})$).\n",
    "This results in a new state $s_{t+1} = (P_{t+1}, x_{t+1})$.\n",
    "\n",
    "\n",
    "### The reward function\n",
    "SeaPearl uses a \"propagation-based reward\". As the goal of the agent is to quickly find a good solution, it needs to learn to effectively prune the search space and move toward promising regions. An intuitive way to configure the reward is to give the agent the objective value, but this information is only available at the end of episodes. In other words, it makes the reward signal extremely sparse. To address this problem, SeaPearl uses both an intermediate reward and a final reward. The intuition behind the intermediate reward is this: it is computed by rewarding the pruning of high values from the variable's domain and penalizing the pruning of low values from the variable's domain. Mathematcially, the intermediate reward is defined as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "r_t^{ub} = \\{ v \\in D_t(x_{\\text{{obj}}}) \\mid v \\notin D_{t+1}(x_{\\text{{obj}}}) \\land v > \\max(D_t(x_{\\text{{obj}}})) \\} \\\\\n",
    "r_t^{lb} = \\{ v \\in D_t(x_{\\text{{obj}}}) \\mid v \\notin D_{t+1}(x_{\\text{{obj}}}) \\land v < \\min(D_t(x_{\\text{{obj}}})) \\} \\\\\n",
    "r^{mid}_t = \\frac{{r_t^{ub} - r_t^{lb}}}{{\\lvert D_1(x_{\\text{{obj}}}) \\rvert}} \\\\\n",
    "r^{end}_t = \\begin{cases} -1 & \\text{{if unfeasible solution found}} \\\\ 0 & \\text{{otherwise}} \\end{cases} \\\\\n",
    "r_{acc} = \\frac{{\\sum_{t=1}^{T} (r^{mid}_t + r^{end}_t)}}{{T-1}}\n",
    "$$\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We will now begin to implement the MIS problem in SeaPearl. To start, we will define the reward, which comes directly from the mathematical definition above. It is implemented in the `GeneralReward` of SeaPearl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.GeneralReward"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward = SeaPearl.GeneralReward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network\n",
    "\n",
    "Next up, we need to define the neural network that will learn the value selection function. As the problems can differ in size, the use of graph neural networks (GNNs) is particularly appropriate. GNNs are a class of neural networks that operate on graphs, which means we need to convert the problem instances to graphs. In SeaPearl, we use tripartite graphs, which are graphs with three types of nodes: variables, values and constraints. There is one node for every variable, for every value and for every constraint. The edges are defined as follows:\n",
    " - There is an edge between a variable and a value if the value is in the domain of the variable.\n",
    " - There is an edge between a variable and a constraint if the variable appears in the constraint.\n",
    "\n",
    "Nodes have the following features:\n",
    " - Values have a one-hot encoding of their value. For example, if the domain of a variable is $\\{1, 2, 3\\}$, then the values will have a one-hot encoding of $[1, 2, 3]$.\n",
    " - Constraints have a one-hot encoding of their type (i.e., constraint).\n",
    "\n",
    "Other features can be used in the graph and we will define a featurization for it later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the experiment\n",
    "\n",
    "In the next cell, we will set up the experiment. We will create structs for the agent and the experiment. We will also define the hyperparameters of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MisExperimentSettings(300, 1, 10, 10, 1, 8, 3, 123)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"MisAgentConfig holds parameters for the configuration of the RL agent that will be used\"\"\"\n",
    "\n",
    "struct MisAgentConfig\n",
    "    gamma::Float32  # Discount factor for future rewards\n",
    "    batch_size::Int  # Batch size\n",
    "    output_size::Int  # One for every node\n",
    "    update_horizon::Int  # How many steps to look ahead in the environment\n",
    "    min_replay_history::Int  # Minimum number of transitions to keep in the replay buffer\n",
    "    update_freq::Int  # Number of actions between successive SGD updates\n",
    "    target_update_freq::Int  # Number of actions between target network updates\n",
    "    trajectory_capacity::Int  # Maximum number of trajectories to store in the replay buffer\n",
    "end\n",
    "\n",
    "\"\"\"MisExperimentSettings holds parameters for the configuration of the experiment\"\"\"\n",
    "struct MisExperimentSettings\n",
    "    nbEpisodes::Int # Number of episodes in the training\n",
    "    restartPerInstances::Int # Number of restarts per instance during training\n",
    "    evalFreq::Int # Number of episodes between evaluations; an evalFreq of 20 means that the performance of heuristics -the agent and the other ones, will be evaluated every 20 episodes\n",
    "    nbInstances::Int # Number of instances to train on\n",
    "    nbRandomHeuristics::Int # Number of random heuristics to use for comparison purposes\n",
    "    nbNewVertices::Int \n",
    "    nbInitialVertices::Int\n",
    "    seedEval::Int # Random seed\n",
    "end\n",
    "\n",
    "agent_config = MisAgentConfig(0.99f0, 64, instance_generator.n, 4, 400, 1, 20, 2000)\n",
    "mis_settings = MisExperimentSettings(300, 1, 10, 10, 1, numNewVertices, numInitialVertices, 123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further configuration\n",
    "\n",
    "Next up, we will define additional configurations for the experiment:\n",
    "- The random seed\n",
    "- Number of steps per episode\n",
    "- The update horizon\n",
    "- The device to use (CPU or GPU)\n",
    "- The evaluation frequency\n",
    "- The steps for the explorer\n",
    "- The parameter initialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_step_per_episode = Int(round(mis_settings.nbNewVertices // 2)) + mis_settings.nbInitialVertices \n",
    "update_horizon = Int(round(n_step_per_episode // 2))\n",
    "device = cpu # change if you have a GPU\n",
    "\n",
    "if device == gpu\n",
    "    CUDA.device!(numDevice)\n",
    "end\n",
    "\n",
    "evalFreq = mis_settings.evalFreq # evaluation frequency, as defined above\n",
    "decay_steps_explorer = Int(floor(mis_settings.nbEpisodes * n_step_per_episode / 2)) # Number of steps after which the explorer will perform exponential decay for epsilon\n",
    "generator = instance_generator # Use the \n",
    "eval_generator = generator\n",
    "\n",
    "rngExp = MersenneTwister(mis_settings.seedEval)\n",
    "init = Flux.glorot_uniform(MersenneTwister(mis_settings.seedEval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The State Representation\n",
    "\n",
    "The state representation is defined in the `HeterogeneousStateRepresentation` class. It is a heterogeneous state representation, which means that it is comprised of multiple state representations. In this example, we will use the `DefaultFeaturization` and the `HeterogeneousTrajectoryState` state representations. The `DefaultFeaturization` state representation is a featurization that is used by default in SeaPearl and allows the user to select the graph features they want. The available features are the following.\n",
    "### Variable Features:\n",
    "- node_number_of_neighbors\n",
    "- variable_initial_domain_size\n",
    "- variable_domain_size\n",
    "- variable_is_bound\n",
    "- variable_is_branchable\n",
    "- variable_is_objective\n",
    "- variable_assigned_value\n",
    "### Constraint Features\n",
    "- node_number_of_neighbors\n",
    "- constraint_activity\n",
    "- nb_involved_constraint_propagation\n",
    "- nb_not_bounded_variable\n",
    "- constraint_type\n",
    "### Value Features\n",
    "- node_number_of_neighbors\n",
    "- values_raw\n",
    "- values_onehot\n",
    "\n",
    "The featurization is used to convert the problem instance to a graph. The `HeterogeneousTrajectoryState` state representation is a state representation that is used to represent the state at a given point in the resolution of the problem. It contains the variable that is branched on, the feature graph at that point and the available values.\n",
    "\n",
    "### Visual representation\n",
    "\n",
    "The following image shows an example of a state representation. Here:\n",
    "- $f_1$ represents features attached to variables ($X_1, X_2, X_3$)\n",
    "- $f_2$ represents features attached to constraints ($C_1, C_2, C_3$)\n",
    "- $f_3$ represents features attached to values (1, 2, 3)\n",
    "\n",
    "<img src=\"img/state-representation.png\" alt=\"State Representation\" style=\"width:700px; height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeaPearl.HeterogeneousStateRepresentation{SeaPearl.DefaultFeaturization, SeaPearl.HeterogeneousTrajectoryState}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defines the features that will be used\n",
    "chosen_features = Dict(\n",
    "    \"node_number_of_neighbors\" => true,\n",
    "    \"constraint_type\" => true,\n",
    "    \"constraint_activity\" => true,\n",
    "    \"nb_not_bounded_variable\" => true,\n",
    "    \"variable_initial_domain_size\" => true,\n",
    "    \"variable_domain_size\" => true,\n",
    "    \"variable_is_objective\" => true,\n",
    "    \"variable_assigned_value\" => true,\n",
    "    \"variable_is_bound\" => true,\n",
    "    \"values_raw\" => true\n",
    ")\n",
    "SR_heterogeneous = SeaPearl.HeterogeneousStateRepresentation{SeaPearl.DefaultFeaturization,SeaPearl.HeterogeneousTrajectoryState}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "In the next cell, we will define the neural network used by the RL agent. We will be using Graph Neural Networks (GNNs) as the learnable architecture. The inputs will be graphs as defined earlier. The features are defined in a Dict and contain elements coming from the problem instance.\n",
    "\n",
    "## The GNN model\n",
    "\n",
    "The model used in this example is drawn from [Marty et al. 2023](https://arxiv.org/abs/2301.01913). The model is defined as a heterogeneous GNN, meaning that every node type uses a specific convolution. The model is comprised of the following elements:\n",
    "\n",
    "### GNN encoder\n",
    "The update equations are given by the following:\n",
    "\\begin{align*}\n",
    "h_{x}^{k+1} &= g\\left(\\theta_{1}^{k}h_{x}^{0} || \\theta_{2}^{k}h_{x}^{k} || \\oplus_{c\\in N_{c}(x)}\\theta_{3}^{k}h_{c}^{k} || \\oplus_{v\\in N_{v}(x)}\\theta_{4}^{k}h_{v}^{k}\\right) \\quad &\\forall x \\in V_1 \\\\\n",
    "h_{c}^{k+1} &= g\\left(\\theta_{5}^{k}h_{c}^{0} || \\theta_{6}^{k}h_{c}^{k} || \\oplus_{x\\in N_{x}(c)}\\theta_{7}^{k}h_{x}^{k}\\right) \\quad &\\forall c \\in V_2 \\\\\n",
    "h_{v}^{k+1} &= g\\left(\\theta_{8}^{k}h_{v}^{0} || \\theta_{9}^{k}h_{v}^{k} || \\oplus_{x\\in N_{x}(v)}\\theta_{10}^{k}h_{x}^{k}\\right) \\quad &\\forall v \\in V_3\n",
    "\\end{align*}\n",
    "\n",
    "- $h_{x}^{k}$, $h_{c}^{k}$, $h_{v}^{k}$ are the feature vectors for nodes of type variable, constraint, and value at layer $k$ respectively.\n",
    "- $h_{x}^{0}$, $h_{c}^{0}$, $h_{v}^{0}$ are the feature vectors for nodes of type variable, constraint, and value from the previous layer. They represent the skip-connections.\n",
    "- $g$ is the leakyReLU activation function which introduces non-linearity into the model.\n",
    "- $\\oplus$ is the mean aggregation function.\n",
    "- $||$ is the concatenation operator.\n",
    "- $\\theta_{i}^{k}$ are weight matrices at layer $k$ which are learned during training.\n",
    "- $N_{c}$, $N_{v}$ and $N_{x}$ are the sets of neighboring nodes for each node.\n",
    "\n",
    "### The action decoder\n",
    "Actions are selected using an epsilon-greedy policy. The Q-values are computed like this:\n",
    "\n",
    "$$\\widehat Q(h_{x}^{K}, h_{v}^{K}) = \\phi_{q}(\\phi_{x}(h_{x}^{K}) \\oplus \\phi_{v}(h_{v}^{K})) \\quad \\forall v \\in V_{x}$$\n",
    "\n",
    "- $h_{x}^{K}$ and $h_{v}^{K}$ are the final node embeddings for the variable and value nodes, respectively, after $K$ iterations in the GNN.\n",
    "- $\\phi_{x}$ and $\\phi_{v}$ are fully connected neural networks that take as input the embeddings of the variable and value nodes, respectively. They further transform these embeddings into an intermediate representation.\n",
    "- $\\oplus$ denotes concatenation of vectors.\n",
    "- $\\phi_{q}$ is another fully connected neural network that takes the concatenated vector as input and outputs a single scalar value, the Q-value.\n",
    "- $V_{x}$ is the subset of value nodes that can be assigned to variable $x$.\n",
    "\n",
    "\n",
    "### Visual Representation\n",
    " \n",
    "Putting it all together, we get the following model:\n",
    "\n",
    "<img src=\"img/High-level-architecture.png\" alt=\"Architecture\" style=\"width:900px; height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Struct that holds the layers of the neural network\"\"\"\n",
    "struct HeterogeneousModel{A,B}\n",
    "    Inputlayer::A\n",
    "    Middlelayers::Vector{B}\n",
    "end\n",
    "\n",
    "Flux.@functor HeterogeneousModel # To allow automatic differentiation\n",
    "\"\"\"Passes feature graphs (instances) through the neural network, by always passing the original feature graph\n",
    "as well as the current feature graph to the layers of the network. \n",
    "\"\"\"\n",
    "function (m::HeterogeneousModel)(fg)\n",
    "    original_fg = deepcopy(fg)\n",
    "    out = m.Inputlayer(fg)\n",
    "    for layer in m.Middlelayers\n",
    "        out = layer(out, original_fg)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# The size of the input features for each type of node (variable, constraint, value), respectively\n",
    "feature_size = [6, 5, 2]\n",
    "\n",
    "\"\"\"\n",
    "    get_dense_chain(in, mid, out, n_layers, σ=Flux.identity; init=Flux.glorot_uniform)\n",
    "\n",
    "Create a chain of dense layers for a neural network.\n",
    "\n",
    "# Arguments\n",
    "- `in::Int`: The size of the input layer.\n",
    "- `mid::Int`: The size of the intermediate layers.\n",
    "- `out::Int`: The size of the output layer.\n",
    "- `n_layers::Int`: The number of layers in the chain.\n",
    "- `σ::Function=Flux.identity`: The activation function to use.\n",
    "- `init::Function=Flux.glorot_uniform`: The initialization method to use.\n",
    "\n",
    "# Returns\n",
    "A `Flux.Chain` object representing the chain of dense layers.\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "julia> get_dense_chain(10, 20, 5, 3)\n",
    "Chain(Dense(10, 20, σ), Dense(20, 20, σ), Dense(20, 5))\n",
    "```\n",
    "\"\"\"\n",
    "function get_dense_chain(in, mid, out, n_layers, σ=Flux.identity; init=Flux.glorot_uniform)\n",
    "    @assert n_layers >= 1\n",
    "    layers = []\n",
    "    if n_layers == 1\n",
    "        push!(layers, Flux.Dense(in, out, init=init))\n",
    "    elseif n_layers == 2\n",
    "        push!(layers, Flux.Dense(in, mid, σ, init=init))\n",
    "        push!(layers, Flux.Dense(mid, out, init=init))\n",
    "    else\n",
    "        push!(layers, Flux.Dense(in, mid, σ, init=init))\n",
    "        for i in 2:(n_layers-1)\n",
    "            push!(layers, Flux.Dense(mid, mid, σ, init=init))\n",
    "        end\n",
    "        push!(layers, Flux.Dense(mid, out, init=init))\n",
    "    end\n",
    "    return Flux.Chain(layers...)\n",
    "end\n",
    "\n",
    "# Builds the SeaPearl HeterogeneousFullFeaturedCPNN model\n",
    "function build_model(; \n",
    "        feature_size,\n",
    "        conv_size=8,\n",
    "        dense_size=16,\n",
    "        output_size=1,\n",
    "        n_layers_graph=3,\n",
    "        n_layers_node=2,\n",
    "        n_layers_output=2,\n",
    "        pool=SeaPearl.meanPooling(),\n",
    "        σ=Flux.leakyrelu,\n",
    "        init=Flux.glorot_uniform,\n",
    "        device=cpu\n",
    "    )\n",
    "    input_layer = SeaPearl.HeterogeneousGraphConvInit(feature_size, conv_size, σ, init=init) # input layer\n",
    "    middle_layers = SeaPearl.HeterogeneousGraphConv[] # middle layers\n",
    "    for i in 1:n_layers_graph-1\n",
    "        push!(middle_layers, SeaPearl.HeterogeneousGraphConv(conv_size => conv_size, feature_size, σ, pool=pool, init=init))\n",
    "    end\n",
    "    output_layer = SeaPearl.HeterogeneousGraphConv(conv_size => output_size, feature_size, σ, pool=pool, init=init) # output layer\n",
    "    dense_layers = get_dense_chain(conv_size, dense_size, dense_size, n_layers_node, σ, init=init) # dense layers\n",
    "    # Define the final output layer\n",
    "    final_output_layer = get_dense_chain(2 * dense_size, dense_size, output_size, n_layers_output, σ, init=init)\n",
    "\n",
    "    # Build the model\n",
    "    model = SeaPearl.HeterogeneousFullFeaturedCPNN(\n",
    "        HeterogeneousModel(input_layer, middle_layers),\n",
    "        dense_layers,\n",
    "        Flux.Chain(),\n",
    "        final_output_layer\n",
    "    ) |> device\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent, Replay Buffer and Exploration Policy\n",
    "\n",
    "We now have:\n",
    "- The reward function\n",
    "- An instance generator\n",
    "- A GNN\n",
    "- And all the settings we need!\n",
    "\n",
    "We now need to define:\n",
    "- The way we will store trajectories will be stored (in a circular buffer)\n",
    "- The exploration policy (eps-greedy)\n",
    "- The agent (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_epsilon_greedy_explorer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    get_heterogeneous_slart_trajectory(; capacity, n_actions)\n",
    "\n",
    "Create a circular buffer for storing trajectories in the context of reinforcement learning where not all actions are legal. \n",
    "SLART stands for State, Legal Actions, Reward, Terminal.\n",
    "\n",
    "# Arguments\n",
    "- `capacity::Int`: The maximum number of trajectories that can be stored in the buffer.\n",
    "- `n_actions::Int`: The number of possible actions that can be taken at each time step.\n",
    "\n",
    "# Returns\n",
    "A `CircularArraySLARTTrajectory` object with the specified capacity and legal actions mask, and an empty state buffer.\n",
    "\"\"\"\n",
    "function get_heterogeneous_slart_trajectory(; capacity, n_actions)\n",
    "    return RL.CircularArraySLARTTrajectory(\n",
    "        capacity=capacity,\n",
    "        state=SeaPearl.HeterogeneousTrajectoryState[] => (),\n",
    "        legal_actions_mask=Vector{Bool} => (n_actions,),\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"Builds the DQN agent for the heterogeneous model\"\"\"\n",
    "function get_heterogeneous_agent(; get_explorer, batch_size=16, update_horizon, min_replay_history, update_freq=1, target_update_freq=200, γ=0.999f0, get_heterogeneous_trajectory, get_heterogeneous_nn)\n",
    "    return RL.Agent(\n",
    "        policy=RL.QBasedPolicy(\n",
    "            learner=get_heterogeneous_learner(batch_size, update_horizon, min_replay_history, update_freq, target_update_freq, get_heterogeneous_nn, γ),\n",
    "            explorer=get_explorer(),\n",
    "        ),\n",
    "        trajectory=get_heterogeneous_trajectory()\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"Builds the learning part of the DQN agent\"\"\"\n",
    "function get_heterogeneous_learner(batch_size, update_horizon, min_replay_history, update_freq, target_update_freq, get_heterogeneous_nn, γ)\n",
    "    return RL.DQNLearner(\n",
    "        approximator=RL.NeuralNetworkApproximator(\n",
    "            model=get_heterogeneous_nn(),\n",
    "            optimizer=ADAM()\n",
    "        ),\n",
    "        target_approximator=RL.NeuralNetworkApproximator(\n",
    "            model=get_heterogeneous_nn(),\n",
    "            optimizer=ADAM()\n",
    "        ),\n",
    "        loss_func=Flux.Losses.huber_loss,\n",
    "        batch_size=batch_size,\n",
    "        update_horizon=update_horizon,\n",
    "        min_replay_history=min_replay_history,\n",
    "        update_freq=update_freq,\n",
    "        target_update_freq=target_update_freq,\n",
    "        γ=γ\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    get_epsilon_greedy_explorer(decay_steps, ϵ_stable; rng=nothing)\n",
    "\n",
    "Create an epsilon-greedy explorer for use in reinforcement learning.\n",
    "\n",
    "# Arguments\n",
    "- `decay_steps::Int`: The number of steps over which to decay the exploration rate.\n",
    "- `ϵ_stable::Real`: The minimum exploration rate to use after decay.\n",
    "- `rng::AbstractRNG`: (optional) A random number generator to use for sampling actions.\n",
    "\n",
    "# Returns\n",
    "An `EpsilonGreedyExplorer` object with the specified exploration rate decay and random number generator.\n",
    "\"\"\"\n",
    "function get_epsilon_greedy_explorer(decay_steps, ϵ_stable; rng=nothing)\n",
    "    if isnothing(rng)\n",
    "        return RL.EpsilonGreedyExplorer(\n",
    "            ϵ_stable=ϵ_stable,\n",
    "            kind=:exp,\n",
    "            decay_steps=decay_steps,\n",
    "            step=1\n",
    "        )\n",
    "    else\n",
    "        return RL.EpsilonGreedyExplorer(\n",
    "            ϵ_stable=ϵ_stable,\n",
    "            kind=:exp,\n",
    "            decay_steps=decay_steps,\n",
    "            step=1,\n",
    "            rng=rng\n",
    "        )\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent definition\n",
    "\n",
    "The agent and the related learned heuristic are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = SeaPearl.meanPooling()\n",
    "\n",
    "agent = get_heterogeneous_agent(;\n",
    "    get_heterogeneous_trajectory=() -> get_heterogeneous_slart_trajectory(capacity=agent_config.trajectory_capacity, n_actions=2),\n",
    "    get_explorer=() -> get_epsilon_greedy_explorer(decay_steps_explorer, 0.01; rng=rngExp),\n",
    "    batch_size=agent_config.batch_size,\n",
    "    update_horizon=update_horizon,\n",
    "    min_replay_history=Int(round(16 * n_step_per_episode // 2)),\n",
    "    update_freq=agent_config.update_freq,\n",
    "    target_update_freq=agent_config.target_update_freq,\n",
    "    get_heterogeneous_nn=() -> build_model(\n",
    "        feature_size=feature_size,\n",
    "        conv_size=8,\n",
    "        dense_size=16,\n",
    "        output_size=1,\n",
    "        n_layers_graph=3,\n",
    "        n_layers_node=3,\n",
    "        n_layers_output=2,\n",
    "        pool=pool,\n",
    "        σ=NNlib.leakyrelu,\n",
    "        init=init,\n",
    "        device=device\n",
    "    ),\n",
    "    γ=0.99f0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up comparisons and running the experiment\n",
    "\n",
    "We now have everything we need to run the experiment. We will run the experiment for 1000 episodes and compare the performance of value selection heuristics:\n",
    "- The learned heuristic\n",
    "- A random agent \n",
    "- Heuristic that always selects the max value available.\n",
    "\n",
    "The agent has already been defined; let's define the other two heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Selects the maximum value from the domain of a variable\"\"\"\n",
    "selectMax(x::SeaPearl.IntVar; cpmodel=nothing) = SeaPearl.maximum(x.domain)\n",
    "\n",
    "\"\"\"Selects a random value from the domain of a variable\"\"\"\n",
    "function select_random_value(x::SeaPearl.IntVar; cpmodel=nothing)\n",
    "    selected_number = rand(1:length(x.domain))\n",
    "    i = 1\n",
    "    for value in x.domain\n",
    "        if i == selected_number\n",
    "            return value\n",
    "        end\n",
    "        i += 1\n",
    "    end\n",
    "    @assert false \"This should not happen\"\n",
    "end\n",
    "\n",
    "randomHeuristics = []\n",
    "for i in 1:mis_settings.nbRandomHeuristics\n",
    "    push!(randomHeuristics, SeaPearl.BasicHeuristic(select_random_value))\n",
    "end\n",
    "\n",
    "heuristic_max = SeaPearl.BasicHeuristic(selectMax)\n",
    "learned_heuristic = SeaPearl.SimpleLearnedHeuristic{SR_heterogeneous,reward,SeaPearl.FixedOutput}(agent; chosen_features=chosen_features)\n",
    "valueSelectionArray = [learned_heuristic, heuristic_max]\n",
    "append!(valueSelectionArray, randomHeuristics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection heuristic\n",
    "\n",
    "For all experiments, we will be using the minimum domain heuristic variable selection heuristic. This heuristic selects the variable with the smallest domain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.MinDomainVariableSelection{false}()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variableSelection = SeaPearl.MinDomainVariableSelection{false}()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the problem\n",
    "\n",
    "Let's finally solve the problem. The following cell will train a DQN agent on MIS instances and evaluate its performance on the same instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsArray, eval_metricsArray = SeaPearl.train!(\n",
    "    valueSelectionArray=valueSelectionArray,\n",
    "    generator=instance_generator,\n",
    "    nbEpisodes=mis_settings.nbEpisodes,\n",
    "    strategy=SeaPearl.DFSearch(),\n",
    "    variableHeuristic=variableSelection,\n",
    "    out_solver=true,\n",
    "    verbose=false,\n",
    "    evaluator=SeaPearl.SameInstancesEvaluator(valueSelectionArray, instance_generator; evalFreq=mis_settings.evalFreq, nbInstances=mis_settings.nbInstances),\n",
    "    restartPerInstances=mis_settings.restartPerInstances\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results\n",
    "\n",
    "Now that we have a small trained model, let's look at the results. First, we will have a look at the performance of the learned heuristic vs random and select max heuristic over the course of the training. Then, we will compare their performance on unseen instances of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Utility functions fetch and organize training metrics + plotting & benchmarking\n",
    "include(\"../../../learning_cp/utils/save_metrics.jl\")\n",
    "include(\"../../../learning_cp/utils/plot_metrics.jl\")\n",
    "include(\"../../../learning_cp/utils/benchmark.jl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training performance\n",
    "\n",
    "In this section, we will be comparing the performance of the learned heuristic with the random and select max heuristics. We will be looking at the performance of the heuristics over the course of the training. The plot in the next cell represents the optimum obtained by the heuristics at every evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = get_metrics_dataframe(eval_metricsArray)\n",
    "plot_first_solution(training_df, 0, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on unseen instances\n",
    "\n",
    "Next up, we will compare the performances of the heuristics on unseen instances. In this section, you are free to play with the various parameters and see their effect. The plot in this section represents the optimum obtained by the heuristics for the first solution they find. For small instances, you should see that the learned heuristic learns something close to the select max heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, Plots\n",
    "\n",
    "validation_generator = SeaPearl.MaximumIndependentSetGenerator(8, 3)\n",
    "\n",
    "num_instances = 20 # Number of instances to evaluate on\n",
    "node_budget = 10000 # Budget of visited nodes\n",
    "take_objective = false # Set it to true if we have to branch on the object ive variable\n",
    "eval_strategy = SeaPearl.DFSearch() #\n",
    "include_dfs = true # Set it to true if you want to evaluate with DFS in addition to ILDS\n",
    "basicHeuristics = Dict()\n",
    "num_random_heuristics = 2\n",
    "\n",
    "for (i, random_heuristic) in enumerate(randomHeuristics)\n",
    "    push!(basicHeuristics, \"random\"*string(i) => random_heuristic)\n",
    "end\n",
    "\n",
    "push!(basicHeuristics, \"max\" => heuristic_max)\n",
    "evaluation_df = benchmark(\n",
    "    models=[agent.policy.learner.approximator], \n",
    "    evaluation_folder=pwd(),\n",
    "    num_instances=num_instances, \n",
    "    chosen_features=chosen_features,\n",
    "    take_objective=take_objective,\n",
    "    generator=validation_generator,\n",
    "    basicHeuristics=basicHeuristics,\n",
    "    save_experiment_metrics=false,\n",
    "    include_dfs=include_dfs, \n",
    "    budget=node_budget,\n",
    "    ILDS=eval_strategy\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing heuristics across 20 instances\n",
    "\n",
    "We will now compare the performance of the learned heuristic with the random and select max heuristics on 20 unseen instances of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the evaluation_df to include only the SimpleLearnedHeuristic\n",
    "learned_heuristic_df = filter(row -> row[:heuristic_type] == \"SimpleLearnedHeuristic\", evaluation_df)\n",
    "# Filter the evaluation_df to include only the BasicHeuristic(selectMax)\n",
    "select_max_df = filter(row -> row[:heuristic_type] == \"BasicHeuristic(selectMax)\", evaluation_df)\n",
    "# Filter the evaluation_df to include only the BasicHeuristic(select_random_value)\n",
    "random_heuristic_df = filter(row -> row[:heuristic_type] == \"BasicHeuristic(select_random_value)\", evaluation_df)\n",
    "\n",
    "# Create a line plot of the first solution for each instance for all heuristics\n",
    "scatter()\n",
    "scatter!(learned_heuristic_df[!, :num_instance], -learned_heuristic_df[!, :first_sol], label=\"SimpleLearnedHeuristic\", markersize=8)\n",
    "scatter!(select_max_df[!, :num_instance], -select_max_df[!, :first_sol], label=\"selectMax\")\n",
    "scatter!(random_heuristic_df[!, :num_instance], -random_heuristic_df[!, :first_sol], label=\"select_random_value\")\n",
    "xlabel!(\"Instance Number\")\n",
    "ylabel!(\"First Solution\")\n",
    "xticks!(1:maximum(evaluation_df[!, :num_instance]), visible=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Large Problems with Pre-Trained Model\n",
    "\n",
    "The last example was on trivially small problems. Now let's have a look at the performance of learned heuristics on larger problems. For this section, we will load a model trained on MIS problems with 40 nodes and we will be comparing the performance of the learned heuristc with random heuristics and the select max heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = SeaPearl.MaximumIndependentSetGenerator(40, 4)\n",
    "num_instances = 20 # Number of instances to evaluate on\n",
    "node_budget = 10000 # Budget of visited nodes\n",
    "take_objective = false # Set it to true if we have to branch on the object ive variable\n",
    "eval_strategy = SeaPearl.DFSearch()\n",
    "include_dfs = true # Set it to true if you want to evaluate with DFS in addition to ILDS\n",
    "basicHeuristics = Dict()\n",
    "num_random_heuristics = 2\n",
    "\n",
    "for (i, random_heuristic) in enumerate(randomHeuristics)\n",
    "    push!(basicHeuristics, \"random\" * string(i) => random_heuristic)\n",
    "end\n",
    "\n",
    "folder = \"./models\"\n",
    "models = []\n",
    "for file in readdir(folder)\n",
    "    if splitext(file)[2] == \".bson\"\n",
    "        println(folder * \"/\" * file)\n",
    "        @load folder * \"/\" * file model\n",
    "        push!(models, model)\n",
    "    end\n",
    "end\n",
    "\n",
    "for (i, random_heuristic) in enumerate(randomHeuristics)\n",
    "    push!(basicHeuristics, \"random\" * string(i) => random_heuristic)\n",
    "end\n",
    "\n",
    "push!(basicHeuristics, \"max\" => heuristic_max)\n",
    "evaluation_df = benchmark(\n",
    "    models=models,\n",
    "    evaluation_folder=pwd(),\n",
    "    num_instances=num_instances,\n",
    "    chosen_features=chosen_features,\n",
    "    take_objective=take_objective,\n",
    "    generator=validation_generator,\n",
    "    basicHeuristics=basicHeuristics,\n",
    "    save_experiment_metrics=false,\n",
    "    include_dfs=include_dfs,\n",
    "    budget=node_budget,\n",
    "    ILDS=eval_strategy\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results\n",
    "\n",
    "We have now loaded a model trained on MIS problems with 40 nodes. Let's have a look at the performance of the learned heuristic vs random and select max heuristic over the course of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the evaluation_df to include only the SimpleLearnedHeuristic\n",
    "learned_heuristic_df = filter(row -> row[:heuristic_type] == \"SimpleLearnedHeuristic\", evaluation_df)\n",
    "# Filter the evaluation_df to include only the BasicHeuristic(selectMax)\n",
    "select_max_df = filter(row -> row[:heuristic_type] == \"BasicHeuristic(selectMax)\", evaluation_df)\n",
    "# Filter the evaluation_df to include only the BasicHeuristic(select_random_value)\n",
    "random_heuristic_df = filter(row -> row[:heuristic_type] == \"BasicHeuristic(select_random_value)\", evaluation_df)\n",
    "\n",
    "# Create a line plot of the first solution for each instance for all heuristics\n",
    "scatter()\n",
    "scatter!(learned_heuristic_df[!, :num_instance], -learned_heuristic_df[!, :first_sol], label=\"SimpleLearnedHeuristic\", markersize=8)\n",
    "scatter!(select_max_df[!, :num_instance], -select_max_df[!, :first_sol], label=\"selectMax\")\n",
    "scatter!(random_heuristic_df[!, :num_instance], -random_heuristic_df[!, :first_sol], label=\"select_random_value\")\n",
    "xlabel!(\"Instance Number\")\n",
    "ylabel!(\"First Solution\")\n",
    "xticks!(1:maximum(evaluation_df[!, :num_instance]), visible=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

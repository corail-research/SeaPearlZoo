{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS\n",
    "In a grph, the maximal independent set, also known as maximal stable set is an independent set that is not a subset of any other independent set. In other words, it is a set of vertices such that no two vertices in the set are adjacent. In this notebook, we will build a learning-based model to find the maximal independent set in a graph.\n",
    "\n",
    "<img src=\"img/Independent_set_graph.png\" alt=\"Solved MIS\" style=\"width:400px; height:400px;\">\n",
    "\n",
    "SeaPearl currently supports learning-based value selection models trained with Reinforcement Learning. To train Reinforcement Learning agents, we start by generating training instances and we let the agent learn a value selection heuristic. Like all other Reinforcement Learning tasks, we need to define a reward function, a state representation and an action space. Finally, we also need to build a neural network that will learn the value selection heuristic. All of these components will be defined in the following sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will begin by activating the environment and importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\leobo\\Desktop\\Ã‰cole\\Poly\\SeaPearl\\SeaPearlZoo.jl`\n"
     ]
    }
   ],
   "source": [
    "using Revise\n",
    "using Pkg\n",
    "Pkg.activate(\"../../../\")\n",
    "using SeaPearl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating instances\n",
    "\n",
    "SeaPearl provides a number of instance generators, including one for the MIS problem. Under the hood, this generator creates Barabasi-Albert graphs with `n` vertices. The graphs are grown by adding new vertices to an initial that has `k` vertices. New vertices are connected by `k` edges to `k` different vertices already present in the system by preferential attachment. The resulting graphs are undirected.\n",
    "\n",
    "<img src=\"img/450px-Barabasi_albert_graph.png\" alt=\"Barabasi-Albert Graph\" style=\"width:600px; height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeaPearl.MaximumIndependentSetGenerator(10, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numInitialVertices = 4\n",
    "numNewVertices = 10\n",
    "instance_generator = SeaPearl.MaximumIndependentSetGenerator(numNewVertices, numInitialVertices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning setup\n",
    "\n",
    "### The state representation\n",
    "In SeaPearl, the state $s_t$ is defined as a pair $s_t = (P_t, x_t)$, with $P_t$ a partially solved combinatorial optimization problem and $x_t$ a variable selected at time $t$ of an episode. A terminal episode is reached if all variables are fixed or if a failure is detected.\n",
    "\n",
    "### The action space\n",
    "Given a state $s_t = (P_t, x_t)$, an action $a_t$ represents the selection of a value $v$ for the variable $x_t$. The action space is defined as the set of all possible values for the variable $x_t$ at time $t$.\n",
    "\n",
    "### The transition function\n",
    "Given a state $s_t = (P_t, x_t)$ and an action $a_t = v$, the transition function is comprised of three steps;\n",
    "1. The value of variable $x_{267}$ is assigned as $v$ (i.e., $D(x_{t+1}) = v$).\n",
    "2. The fix-point operation is applied on $P_t$ to prune the domains (i.e., $P_{t+1} = \\text{{fixPoint}}(P_t)$).\n",
    "3. The next variable to branch on is selected (i.e., $x_{t+1} = \\text{{nextVariable}}(P_{t+1})$).\n",
    "This results in a new state $s_{t+1} = (P_{t+1}, x_{t+1})$.\n",
    "\n",
    "\n",
    "### The reward function\n",
    "SeaPearl uses a \"propagation-based reward\". As the goal of the agent is to quickly find a good solution, it needs to learn to effectively prune the search space and move toward promising regions. An intuitive way to configure the reward is to give the agent the objective value, but this information is only available at the end of episodes. In other words, it makes the reward signal extremely sparse. To address this problem, SeaPearl uses both an intermediate reward and a final reward. The intuition behind the intermediate reward is this: it is computed by rewarding the pruning of high values from the variable's domain and penalizing the pruning of low values from the variable's domain. Mathematcially, the intermediate reward is defined as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "r_t^{ub} = \\{ v \\in D_t(x_{\\text{{obj}}}) \\mid v \\notin D_{t+1}(x_{\\text{{obj}}}) \\land v > \\max(D_t(x_{\\text{{obj}}})) \\} \\\\\n",
    "r_t^{lb} = \\{ v \\in D_t(x_{\\text{{obj}}}) \\mid v \\notin D_{t+1}(x_{\\text{{obj}}}) \\land v < \\min(D_t(x_{\\text{{obj}}})) \\} \\\\\n",
    "r^{mid}_t = \\frac{{r_t^{ub} - r_t^{lb}}}{{\\lvert D_1(x_{\\text{{obj}}}) \\rvert}} \\\\\n",
    "r^{end}_t = \\begin{cases} -1 & \\text{{if unfeasible solution found}} \\\\ 0 & \\text{{otherwise}} \\end{cases} \\\\\n",
    "r_{acc} = \\frac{{\\sum_{t=1}^{T} (r^{mid}_t + r^{end}_t)}}{{T-1}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
